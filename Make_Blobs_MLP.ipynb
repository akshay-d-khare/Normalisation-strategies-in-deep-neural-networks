{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Make_Blobs MLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRQReunBCZhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scatter plot of the circles dataset with points colored by class\n",
        "from sklearn.datasets import make_blobs\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "# generate 2d classification dataset\n",
        "X, y = make_blobs(n_samples=1000, centers=5, n_features=30)\n",
        "y = OneHotEncoder().fit_transform(np.reshape(y, (1000, 1))).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKAezkGhKSIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_train = 900\n",
        "train_X, test_X = X[:n_train, :], X[n_train:, :]\n",
        "train_y, test_y = y[:n_train], y[n_train:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdysxFzqNYrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = train_y.shape[1]\n",
        "num_features = train_X.shape[1]\n",
        "num_output = train_y.shape[1]\n",
        "num_layers_0 = 5\n",
        "num_layers_1 = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Placeholders for the input data\n",
        "input_X = tf.placeholder('float32',shape =(None,num_features),name=\"input_X\")\n",
        "input_y = tf.placeholder('float32',shape = (None,num_classes),name='input_Y')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tFSnXsWeeq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Weights initialized by random normal function with std_dev = 1/sqrt(number of input features)\n",
        "weights_0 = tf.get_variable('W0', shape=(num_features,num_layers_0), initializer=tf.initializers.he_uniform())\n",
        "#weights_0 = tf.Variable(tf.random_normal([], stddev=(1/tf.sqrt(float(num_features)))))\n",
        "bias_0 = tf.get_variable('B0', shape = (num_layers_0), initializer = tf.initializers.he_uniform()) \n",
        "weights_1 = tf.get_variable('W1', shape = (num_layers_0,num_layers_1), initializer = tf.initializers.he_uniform())\n",
        "bias_1 = tf.get_variable('B1', shape = (num_layers_1), initializer = tf.initializers.he_uniform())\n",
        "weights_2 = tf.get_variable('W2', shape = (num_layers_1,num_output), initializer = tf.initializers.he_uniform())\n",
        "bias_2 = tf.get_variable('B2', shape = (num_output), initializer = tf.initializers.he_uniform())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8sEWNFmKzZDx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "8345420c-683d-44e1-858c-6012ab76b0d5"
      },
      "source": [
        "# Initializing weigths and biases\n",
        "hidden_output_0 = tf.matmul(input_X,weights_0)+bias_0\n",
        "\n",
        "hidden_output_0 = tf.contrib.layers.layer_norm(hidden_output_0)\n",
        "\n",
        "hidden_output_0 = tf.nn.relu(hidden_output_0)\n",
        "\n",
        "hidden_output_1 = tf.matmul(hidden_output_0,weights_1)+bias_1\n",
        "\n",
        "hidden_output_1 = tf.contrib.layers.layer_norm(hidden_output_1)\n",
        "\n",
        "hidden_output_1 = tf.nn.relu(hidden_output_1)\n",
        "\n",
        "predicted_y = tf.sigmoid(tf.matmul(hidden_output_1,weights_2) + bias_2)\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predicted_y,labels=input_y))\n",
        "\n",
        "## Adam optimzer for finding the right weight\n",
        "optimizer = tf.train.MomentumOptimizer(0.001, momentum = 0.9).minimize(cost)\n",
        "\n",
        "## Metrics definition\n",
        "correct_prediction = tf.equal(tf.argmax(predicted_y, 1), tf.argmax(input_y, 1))\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0828 05:59:22.265884 139778491058048 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5UVaJr8mopE",
        "colab_type": "code",
        "outputId": "8f81de9d-c3cd-4d11-f0e0-8b88ccd24ab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## Training parameters\n",
        "batch_size = 64\n",
        "training_iters=100\n",
        "training_accuracy = []\n",
        "training_loss = []\n",
        "testing_accuracy = []\n",
        "testing_loss = []\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n",
        "\n",
        "  for i in range(training_iters):\n",
        "      for batch in range(len(train_X)//batch_size):\n",
        "          batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]\n",
        "          batch_y = train_y[batch*batch_size:min((batch+1)*batch_size,len(train_y))]    \n",
        "            # Run optimization op (backprop).\n",
        "                # Calculate batch loss and accuracy\n",
        "          opt = sess.run(optimizer, feed_dict={input_X: batch_x,\n",
        "                                                              input_y: batch_y})\n",
        "          loss, acc = sess.run([cost, accuracy], feed_dict={input_X: batch_x,\n",
        "                                                              input_y: batch_y})\n",
        "\n",
        "          print(\"Iter \" + str(i) + \", Loss= \" + \\\n",
        "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                      \"{:.5f}\".format(acc))\n",
        "      print(\"Optimization Finished!\")\n",
        "\n",
        "        # Calculate accuracy for all test images\n",
        "      test_acc,valid_loss = sess.run([accuracy,cost], feed_dict={input_X: test_X,input_y : test_y})\n",
        "      training_loss.append(loss)\n",
        "      testing_loss.append(valid_loss)\n",
        "      training_accuracy.append(acc)\n",
        "      testing_accuracy.append(test_acc)\n",
        "      print(\"Testing Accuracy:\",\"{:.5f}\".format(test_acc))\n",
        "  summary_writer.close()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter 0, Loss= 1.699161, Training Accuracy= 0.00000\n",
            "Iter 0, Loss= 1.679464, Training Accuracy= 0.00000\n",
            "Iter 0, Loss= 1.679600, Training Accuracy= 0.00000\n",
            "Iter 0, Loss= 1.664285, Training Accuracy= 0.00000\n",
            "Iter 0, Loss= 1.699157, Training Accuracy= 0.00000\n",
            "Iter 0, Loss= 1.682156, Training Accuracy= 0.00000\n",
            "Iter 0, Loss= 1.698353, Training Accuracy= 0.00000\n",
            "Iter 0, Loss= 1.690999, Training Accuracy= 0.00000\n",
            "Iter 0, Loss= 1.695643, Training Accuracy= 0.00000\n",
            "Iter 0, Loss= 1.673902, Training Accuracy= 0.00000\n",
            "Iter 0, Loss= 1.693681, Training Accuracy= 0.00000\n",
            "Iter 0, Loss= 1.658249, Training Accuracy= 0.00000\n",
            "Iter 0, Loss= 1.686961, Training Accuracy= 0.00000\n",
            "Iter 0, Loss= 1.666232, Training Accuracy= 0.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.00000\n",
            "Iter 1, Loss= 1.694485, Training Accuracy= 0.00000\n",
            "Iter 1, Loss= 1.674127, Training Accuracy= 0.00000\n",
            "Iter 1, Loss= 1.674049, Training Accuracy= 0.00000\n",
            "Iter 1, Loss= 1.658849, Training Accuracy= 0.00000\n",
            "Iter 1, Loss= 1.693568, Training Accuracy= 0.00000\n",
            "Iter 1, Loss= 1.675612, Training Accuracy= 0.00000\n",
            "Iter 1, Loss= 1.692772, Training Accuracy= 0.00000\n",
            "Iter 1, Loss= 1.684527, Training Accuracy= 0.00000\n",
            "Iter 1, Loss= 1.688288, Training Accuracy= 0.00000\n",
            "Iter 1, Loss= 1.667554, Training Accuracy= 0.00000\n",
            "Iter 1, Loss= 1.687977, Training Accuracy= 0.00000\n",
            "Iter 1, Loss= 1.651446, Training Accuracy= 0.00000\n",
            "Iter 1, Loss= 1.680555, Training Accuracy= 0.00000\n",
            "Iter 1, Loss= 1.659664, Training Accuracy= 0.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.00000\n",
            "Iter 2, Loss= 1.686600, Training Accuracy= 0.00000\n",
            "Iter 2, Loss= 1.665976, Training Accuracy= 0.00000\n",
            "Iter 2, Loss= 1.665782, Training Accuracy= 0.00000\n",
            "Iter 2, Loss= 1.651565, Training Accuracy= 0.00000\n",
            "Iter 2, Loss= 1.685855, Training Accuracy= 0.00000\n",
            "Iter 2, Loss= 1.667215, Training Accuracy= 0.00000\n",
            "Iter 2, Loss= 1.685709, Training Accuracy= 0.00000\n",
            "Iter 2, Loss= 1.676323, Training Accuracy= 0.00000\n",
            "Iter 2, Loss= 1.678870, Training Accuracy= 0.00000\n",
            "Iter 2, Loss= 1.660275, Training Accuracy= 0.00000\n",
            "Iter 2, Loss= 1.680978, Training Accuracy= 0.00000\n",
            "Iter 2, Loss= 1.643800, Training Accuracy= 0.00000\n",
            "Iter 2, Loss= 1.673200, Training Accuracy= 0.00000\n",
            "Iter 2, Loss= 1.652276, Training Accuracy= 0.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.00000\n",
            "Iter 3, Loss= 1.677445, Training Accuracy= 0.00000\n",
            "Iter 3, Loss= 1.657150, Training Accuracy= 0.00000\n",
            "Iter 3, Loss= 1.656368, Training Accuracy= 0.00000\n",
            "Iter 3, Loss= 1.643600, Training Accuracy= 0.00000\n",
            "Iter 3, Loss= 1.677322, Training Accuracy= 0.00000\n",
            "Iter 3, Loss= 1.657923, Training Accuracy= 0.00000\n",
            "Iter 3, Loss= 1.677623, Training Accuracy= 0.00000\n",
            "Iter 3, Loss= 1.667308, Training Accuracy= 0.00000\n",
            "Iter 3, Loss= 1.668397, Training Accuracy= 0.00000\n",
            "Iter 3, Loss= 1.652566, Training Accuracy= 0.00000\n",
            "Iter 3, Loss= 1.673288, Training Accuracy= 0.00000\n",
            "Iter 3, Loss= 1.636015, Training Accuracy= 0.00000\n",
            "Iter 3, Loss= 1.665609, Training Accuracy= 0.00000\n",
            "Iter 3, Loss= 1.644439, Training Accuracy= 0.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.00000\n",
            "Iter 4, Loss= 1.667475, Training Accuracy= 0.00000\n",
            "Iter 4, Loss= 1.647679, Training Accuracy= 0.00000\n",
            "Iter 4, Loss= 1.646252, Training Accuracy= 0.00000\n",
            "Iter 4, Loss= 1.635372, Training Accuracy= 0.00000\n",
            "Iter 4, Loss= 1.667843, Training Accuracy= 0.00000\n",
            "Iter 4, Loss= 1.648779, Training Accuracy= 0.00000\n",
            "Iter 4, Loss= 1.668454, Training Accuracy= 0.00000\n",
            "Iter 4, Loss= 1.657622, Training Accuracy= 0.00000\n",
            "Iter 4, Loss= 1.657909, Training Accuracy= 0.00000\n",
            "Iter 4, Loss= 1.644727, Training Accuracy= 0.00000\n",
            "Iter 4, Loss= 1.665330, Training Accuracy= 0.00000\n",
            "Iter 4, Loss= 1.627999, Training Accuracy= 0.00000\n",
            "Iter 4, Loss= 1.657586, Training Accuracy= 0.00000\n",
            "Iter 4, Loss= 1.636198, Training Accuracy= 0.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.00000\n",
            "Iter 5, Loss= 1.656712, Training Accuracy= 0.00000\n",
            "Iter 5, Loss= 1.637827, Training Accuracy= 0.00000\n",
            "Iter 5, Loss= 1.635899, Training Accuracy= 0.00000\n",
            "Iter 5, Loss= 1.627316, Training Accuracy= 0.00000\n",
            "Iter 5, Loss= 1.657854, Training Accuracy= 0.00000\n",
            "Iter 5, Loss= 1.640030, Training Accuracy= 0.00000\n",
            "Iter 5, Loss= 1.658845, Training Accuracy= 0.00000\n",
            "Iter 5, Loss= 1.647919, Training Accuracy= 0.00000\n",
            "Iter 5, Loss= 1.647429, Training Accuracy= 0.00000\n",
            "Iter 5, Loss= 1.636652, Training Accuracy= 0.00000\n",
            "Iter 5, Loss= 1.657130, Training Accuracy= 0.00000\n",
            "Iter 5, Loss= 1.619964, Training Accuracy= 0.00000\n",
            "Iter 5, Loss= 1.649405, Training Accuracy= 0.00000\n",
            "Iter 5, Loss= 1.627769, Training Accuracy= 0.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.00000\n",
            "Iter 6, Loss= 1.645653, Training Accuracy= 0.00000\n",
            "Iter 6, Loss= 1.627708, Training Accuracy= 0.00000\n",
            "Iter 6, Loss= 1.625586, Training Accuracy= 0.00000\n",
            "Iter 6, Loss= 1.619777, Training Accuracy= 0.00000\n",
            "Iter 6, Loss= 1.647559, Training Accuracy= 0.00000\n",
            "Iter 6, Loss= 1.631440, Training Accuracy= 0.00000\n",
            "Iter 6, Loss= 1.649549, Training Accuracy= 0.00000\n",
            "Iter 6, Loss= 1.638453, Training Accuracy= 0.00000\n",
            "Iter 6, Loss= 1.637267, Training Accuracy= 0.00000\n",
            "Iter 6, Loss= 1.628793, Training Accuracy= 0.00000\n",
            "Iter 6, Loss= 1.649192, Training Accuracy= 0.00000\n",
            "Iter 6, Loss= 1.612094, Training Accuracy= 0.00000\n",
            "Iter 6, Loss= 1.640953, Training Accuracy= 0.00000\n",
            "Iter 6, Loss= 1.619501, Training Accuracy= 0.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.00000\n",
            "Iter 7, Loss= 1.635131, Training Accuracy= 0.00000\n",
            "Iter 7, Loss= 1.617947, Training Accuracy= 0.00000\n",
            "Iter 7, Loss= 1.616207, Training Accuracy= 0.00000\n",
            "Iter 7, Loss= 1.612630, Training Accuracy= 0.00000\n",
            "Iter 7, Loss= 1.637201, Training Accuracy= 0.00000\n",
            "Iter 7, Loss= 1.623124, Training Accuracy= 0.00000\n",
            "Iter 7, Loss= 1.640426, Training Accuracy= 0.00000\n",
            "Iter 7, Loss= 1.629661, Training Accuracy= 0.00000\n",
            "Iter 7, Loss= 1.627607, Training Accuracy= 0.00000\n",
            "Iter 7, Loss= 1.621052, Training Accuracy= 0.01562\n",
            "Iter 7, Loss= 1.641448, Training Accuracy= 0.00000\n",
            "Iter 7, Loss= 1.604896, Training Accuracy= 0.00000\n",
            "Iter 7, Loss= 1.632761, Training Accuracy= 0.00000\n",
            "Iter 7, Loss= 1.611539, Training Accuracy= 0.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.00000\n",
            "Iter 8, Loss= 1.625412, Training Accuracy= 0.00000\n",
            "Iter 8, Loss= 1.608572, Training Accuracy= 0.01562\n",
            "Iter 8, Loss= 1.607170, Training Accuracy= 0.00000\n",
            "Iter 8, Loss= 1.605733, Training Accuracy= 0.00000\n",
            "Iter 8, Loss= 1.627039, Training Accuracy= 0.00000\n",
            "Iter 8, Loss= 1.614780, Training Accuracy= 0.00000\n",
            "Iter 8, Loss= 1.631247, Training Accuracy= 0.00000\n",
            "Iter 8, Loss= 1.620902, Training Accuracy= 0.00000\n",
            "Iter 8, Loss= 1.618380, Training Accuracy= 0.00000\n",
            "Iter 8, Loss= 1.613557, Training Accuracy= 0.01562\n",
            "Iter 8, Loss= 1.634009, Training Accuracy= 0.00000\n",
            "Iter 8, Loss= 1.598029, Training Accuracy= 0.00000\n",
            "Iter 8, Loss= 1.624552, Training Accuracy= 0.00000\n",
            "Iter 8, Loss= 1.603608, Training Accuracy= 0.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.01000\n",
            "Iter 9, Loss= 1.615832, Training Accuracy= 0.00000\n",
            "Iter 9, Loss= 1.599592, Training Accuracy= 0.03125\n",
            "Iter 9, Loss= 1.598251, Training Accuracy= 0.01562\n",
            "Iter 9, Loss= 1.598768, Training Accuracy= 0.00000\n",
            "Iter 9, Loss= 1.616565, Training Accuracy= 0.00000\n",
            "Iter 9, Loss= 1.606241, Training Accuracy= 0.00000\n",
            "Iter 9, Loss= 1.622113, Training Accuracy= 0.00000\n",
            "Iter 9, Loss= 1.611789, Training Accuracy= 0.00000\n",
            "Iter 9, Loss= 1.608970, Training Accuracy= 0.03125\n",
            "Iter 9, Loss= 1.605705, Training Accuracy= 0.01562\n",
            "Iter 9, Loss= 1.626393, Training Accuracy= 0.00000\n",
            "Iter 9, Loss= 1.591163, Training Accuracy= 0.00000\n",
            "Iter 9, Loss= 1.615681, Training Accuracy= 0.00000\n",
            "Iter 9, Loss= 1.595704, Training Accuracy= 0.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.01000\n",
            "Iter 10, Loss= 1.605650, Training Accuracy= 0.00000\n",
            "Iter 10, Loss= 1.590353, Training Accuracy= 0.04688\n",
            "Iter 10, Loss= 1.588906, Training Accuracy= 0.06250\n",
            "Iter 10, Loss= 1.591470, Training Accuracy= 0.01562\n",
            "Iter 10, Loss= 1.605508, Training Accuracy= 0.00000\n",
            "Iter 10, Loss= 1.596979, Training Accuracy= 0.00000\n",
            "Iter 10, Loss= 1.612518, Training Accuracy= 0.00000\n",
            "Iter 10, Loss= 1.602221, Training Accuracy= 0.03125\n",
            "Iter 10, Loss= 1.598536, Training Accuracy= 0.04688\n",
            "Iter 10, Loss= 1.597498, Training Accuracy= 0.03125\n",
            "Iter 10, Loss= 1.618240, Training Accuracy= 0.00000\n",
            "Iter 10, Loss= 1.583976, Training Accuracy= 0.01562\n",
            "Iter 10, Loss= 1.605593, Training Accuracy= 0.00000\n",
            "Iter 10, Loss= 1.587093, Training Accuracy= 0.01562\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.04000\n",
            "Iter 11, Loss= 1.594482, Training Accuracy= 0.01562\n",
            "Iter 11, Loss= 1.579902, Training Accuracy= 0.04688\n",
            "Iter 11, Loss= 1.578646, Training Accuracy= 0.06250\n",
            "Iter 11, Loss= 1.583542, Training Accuracy= 0.03125\n",
            "Iter 11, Loss= 1.593654, Training Accuracy= 0.01562\n",
            "Iter 11, Loss= 1.586809, Training Accuracy= 0.00000\n",
            "Iter 11, Loss= 1.601721, Training Accuracy= 0.00000\n",
            "Iter 11, Loss= 1.591271, Training Accuracy= 0.06250\n",
            "Iter 11, Loss= 1.586504, Training Accuracy= 0.04688\n",
            "Iter 11, Loss= 1.588382, Training Accuracy= 0.06250\n",
            "Iter 11, Loss= 1.608835, Training Accuracy= 0.01562\n",
            "Iter 11, Loss= 1.576157, Training Accuracy= 0.03125\n",
            "Iter 11, Loss= 1.594641, Training Accuracy= 0.00000\n",
            "Iter 11, Loss= 1.576989, Training Accuracy= 0.04688\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.06000\n",
            "Iter 12, Loss= 1.581473, Training Accuracy= 0.03125\n",
            "Iter 12, Loss= 1.567804, Training Accuracy= 0.04688\n",
            "Iter 12, Loss= 1.566958, Training Accuracy= 0.06250\n",
            "Iter 12, Loss= 1.574935, Training Accuracy= 0.06250\n",
            "Iter 12, Loss= 1.580963, Training Accuracy= 0.04688\n",
            "Iter 12, Loss= 1.575321, Training Accuracy= 0.00000\n",
            "Iter 12, Loss= 1.589162, Training Accuracy= 0.00000\n",
            "Iter 12, Loss= 1.578962, Training Accuracy= 0.07812\n",
            "Iter 12, Loss= 1.573253, Training Accuracy= 0.07812\n",
            "Iter 12, Loss= 1.578411, Training Accuracy= 0.06250\n",
            "Iter 12, Loss= 1.597879, Training Accuracy= 0.01562\n",
            "Iter 12, Loss= 1.567358, Training Accuracy= 0.06250\n",
            "Iter 12, Loss= 1.582692, Training Accuracy= 0.04688\n",
            "Iter 12, Loss= 1.566290, Training Accuracy= 0.07812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.07000\n",
            "Iter 13, Loss= 1.566885, Training Accuracy= 0.06250\n",
            "Iter 13, Loss= 1.554808, Training Accuracy= 0.10938\n",
            "Iter 13, Loss= 1.554381, Training Accuracy= 0.14062\n",
            "Iter 13, Loss= 1.565902, Training Accuracy= 0.14062\n",
            "Iter 13, Loss= 1.567691, Training Accuracy= 0.07812\n",
            "Iter 13, Loss= 1.562783, Training Accuracy= 0.04688\n",
            "Iter 13, Loss= 1.575764, Training Accuracy= 0.03125\n",
            "Iter 13, Loss= 1.566169, Training Accuracy= 0.09375\n",
            "Iter 13, Loss= 1.559624, Training Accuracy= 0.12500\n",
            "Iter 13, Loss= 1.567889, Training Accuracy= 0.06250\n",
            "Iter 13, Loss= 1.586521, Training Accuracy= 0.03125\n",
            "Iter 13, Loss= 1.557805, Training Accuracy= 0.10938\n",
            "Iter 13, Loss= 1.570373, Training Accuracy= 0.04688\n",
            "Iter 13, Loss= 1.555452, Training Accuracy= 0.14062\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.09000\n",
            "Iter 14, Loss= 1.552084, Training Accuracy= 0.12500\n",
            "Iter 14, Loss= 1.541766, Training Accuracy= 0.14062\n",
            "Iter 14, Loss= 1.541826, Training Accuracy= 0.15625\n",
            "Iter 14, Loss= 1.556896, Training Accuracy= 0.15625\n",
            "Iter 14, Loss= 1.554482, Training Accuracy= 0.12500\n",
            "Iter 14, Loss= 1.550211, Training Accuracy= 0.09375\n",
            "Iter 14, Loss= 1.562222, Training Accuracy= 0.04688\n",
            "Iter 14, Loss= 1.553506, Training Accuracy= 0.10938\n",
            "Iter 14, Loss= 1.546105, Training Accuracy= 0.15625\n",
            "Iter 14, Loss= 1.557469, Training Accuracy= 0.09375\n",
            "Iter 14, Loss= 1.575212, Training Accuracy= 0.04688\n",
            "Iter 14, Loss= 1.548334, Training Accuracy= 0.15625\n",
            "Iter 14, Loss= 1.558170, Training Accuracy= 0.09375\n",
            "Iter 14, Loss= 1.544878, Training Accuracy= 0.14062\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.13000\n",
            "Iter 15, Loss= 1.537590, Training Accuracy= 0.15625\n",
            "Iter 15, Loss= 1.529005, Training Accuracy= 0.15625\n",
            "Iter 15, Loss= 1.529516, Training Accuracy= 0.17188\n",
            "Iter 15, Loss= 1.548131, Training Accuracy= 0.17188\n",
            "Iter 15, Loss= 1.541602, Training Accuracy= 0.17188\n",
            "Iter 15, Loss= 1.537873, Training Accuracy= 0.18750\n",
            "Iter 15, Loss= 1.548946, Training Accuracy= 0.10938\n",
            "Iter 15, Loss= 1.541152, Training Accuracy= 0.12500\n",
            "Iter 15, Loss= 1.532918, Training Accuracy= 0.20312\n",
            "Iter 15, Loss= 1.547256, Training Accuracy= 0.18750\n",
            "Iter 15, Loss= 1.564070, Training Accuracy= 0.04688\n",
            "Iter 15, Loss= 1.539095, Training Accuracy= 0.21875\n",
            "Iter 15, Loss= 1.546225, Training Accuracy= 0.12500\n",
            "Iter 15, Loss= 1.534553, Training Accuracy= 0.17188\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.20000\n",
            "Iter 16, Loss= 1.523481, Training Accuracy= 0.21875\n",
            "Iter 16, Loss= 1.516657, Training Accuracy= 0.23438\n",
            "Iter 16, Loss= 1.517602, Training Accuracy= 0.25000\n",
            "Iter 16, Loss= 1.539569, Training Accuracy= 0.26562\n",
            "Iter 16, Loss= 1.529134, Training Accuracy= 0.26562\n",
            "Iter 16, Loss= 1.525875, Training Accuracy= 0.29688\n",
            "Iter 16, Loss= 1.536033, Training Accuracy= 0.18750\n",
            "Iter 16, Loss= 1.529154, Training Accuracy= 0.23438\n",
            "Iter 16, Loss= 1.520122, Training Accuracy= 0.37500\n",
            "Iter 16, Loss= 1.537306, Training Accuracy= 0.25000\n",
            "Iter 16, Loss= 1.553247, Training Accuracy= 0.14062\n",
            "Iter 16, Loss= 1.530060, Training Accuracy= 0.31250\n",
            "Iter 16, Loss= 1.534632, Training Accuracy= 0.23438\n",
            "Iter 16, Loss= 1.524639, Training Accuracy= 0.28125\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.37000\n",
            "Iter 17, Loss= 1.509874, Training Accuracy= 0.32812\n",
            "Iter 17, Loss= 1.504775, Training Accuracy= 0.37500\n",
            "Iter 17, Loss= 1.506160, Training Accuracy= 0.35938\n",
            "Iter 17, Loss= 1.531242, Training Accuracy= 0.31250\n",
            "Iter 17, Loss= 1.517214, Training Accuracy= 0.32812\n",
            "Iter 17, Loss= 1.514266, Training Accuracy= 0.35938\n",
            "Iter 17, Loss= 1.523557, Training Accuracy= 0.29688\n",
            "Iter 17, Loss= 1.517654, Training Accuracy= 0.39062\n",
            "Iter 17, Loss= 1.507869, Training Accuracy= 0.45312\n",
            "Iter 17, Loss= 1.527655, Training Accuracy= 0.29688\n",
            "Iter 17, Loss= 1.542871, Training Accuracy= 0.25000\n",
            "Iter 17, Loss= 1.521305, Training Accuracy= 0.31250\n",
            "Iter 17, Loss= 1.523441, Training Accuracy= 0.31250\n",
            "Iter 17, Loss= 1.515157, Training Accuracy= 0.34375\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.43000\n",
            "Iter 18, Loss= 1.496865, Training Accuracy= 0.46875\n",
            "Iter 18, Loss= 1.493415, Training Accuracy= 0.46875\n",
            "Iter 18, Loss= 1.495259, Training Accuracy= 0.45312\n",
            "Iter 18, Loss= 1.523238, Training Accuracy= 0.32812\n",
            "Iter 18, Loss= 1.505843, Training Accuracy= 0.40625\n",
            "Iter 18, Loss= 1.503114, Training Accuracy= 0.40625\n",
            "Iter 18, Loss= 1.511584, Training Accuracy= 0.35938\n",
            "Iter 18, Loss= 1.506787, Training Accuracy= 0.40625\n",
            "Iter 18, Loss= 1.496222, Training Accuracy= 0.45312\n",
            "Iter 18, Loss= 1.518345, Training Accuracy= 0.32812\n",
            "Iter 18, Loss= 1.532932, Training Accuracy= 0.28125\n",
            "Iter 18, Loss= 1.512893, Training Accuracy= 0.35938\n",
            "Iter 18, Loss= 1.512769, Training Accuracy= 0.35938\n",
            "Iter 18, Loss= 1.506079, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 19, Loss= 1.484522, Training Accuracy= 0.46875\n",
            "Iter 19, Loss= 1.482551, Training Accuracy= 0.46875\n",
            "Iter 19, Loss= 1.484838, Training Accuracy= 0.45312\n",
            "Iter 19, Loss= 1.515494, Training Accuracy= 0.34375\n",
            "Iter 19, Loss= 1.495173, Training Accuracy= 0.42188\n",
            "Iter 19, Loss= 1.492459, Training Accuracy= 0.40625\n",
            "Iter 19, Loss= 1.500138, Training Accuracy= 0.37500\n",
            "Iter 19, Loss= 1.496381, Training Accuracy= 0.40625\n",
            "Iter 19, Loss= 1.485189, Training Accuracy= 0.45312\n",
            "Iter 19, Loss= 1.509393, Training Accuracy= 0.32812\n",
            "Iter 19, Loss= 1.523429, Training Accuracy= 0.29688\n",
            "Iter 19, Loss= 1.504803, Training Accuracy= 0.35938\n",
            "Iter 19, Loss= 1.502631, Training Accuracy= 0.35938\n",
            "Iter 19, Loss= 1.497363, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 20, Loss= 1.472760, Training Accuracy= 0.46875\n",
            "Iter 20, Loss= 1.472249, Training Accuracy= 0.46875\n",
            "Iter 20, Loss= 1.474866, Training Accuracy= 0.45312\n",
            "Iter 20, Loss= 1.508010, Training Accuracy= 0.34375\n",
            "Iter 20, Loss= 1.485104, Training Accuracy= 0.42188\n",
            "Iter 20, Loss= 1.482440, Training Accuracy= 0.40625\n",
            "Iter 20, Loss= 1.489350, Training Accuracy= 0.37500\n",
            "Iter 20, Loss= 1.486472, Training Accuracy= 0.40625\n",
            "Iter 20, Loss= 1.474719, Training Accuracy= 0.45312\n",
            "Iter 20, Loss= 1.500849, Training Accuracy= 0.32812\n",
            "Iter 20, Loss= 1.514367, Training Accuracy= 0.29688\n",
            "Iter 20, Loss= 1.496988, Training Accuracy= 0.37500\n",
            "Iter 20, Loss= 1.492996, Training Accuracy= 0.35938\n",
            "Iter 20, Loss= 1.489016, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 21, Loss= 1.461671, Training Accuracy= 0.46875\n",
            "Iter 21, Loss= 1.462434, Training Accuracy= 0.46875\n",
            "Iter 21, Loss= 1.465366, Training Accuracy= 0.45312\n",
            "Iter 21, Loss= 1.500826, Training Accuracy= 0.34375\n",
            "Iter 21, Loss= 1.475571, Training Accuracy= 0.42188\n",
            "Iter 21, Loss= 1.472886, Training Accuracy= 0.40625\n",
            "Iter 21, Loss= 1.479140, Training Accuracy= 0.39062\n",
            "Iter 21, Loss= 1.476988, Training Accuracy= 0.40625\n",
            "Iter 21, Loss= 1.464784, Training Accuracy= 0.45312\n",
            "Iter 21, Loss= 1.492638, Training Accuracy= 0.32812\n",
            "Iter 21, Loss= 1.505766, Training Accuracy= 0.29688\n",
            "Iter 21, Loss= 1.489438, Training Accuracy= 0.37500\n",
            "Iter 21, Loss= 1.483873, Training Accuracy= 0.35938\n",
            "Iter 21, Loss= 1.480999, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 22, Loss= 1.451315, Training Accuracy= 0.46875\n",
            "Iter 22, Loss= 1.453077, Training Accuracy= 0.46875\n",
            "Iter 22, Loss= 1.456295, Training Accuracy= 0.45312\n",
            "Iter 22, Loss= 1.493934, Training Accuracy= 0.34375\n",
            "Iter 22, Loss= 1.466465, Training Accuracy= 0.42188\n",
            "Iter 22, Loss= 1.463746, Training Accuracy= 0.40625\n",
            "Iter 22, Loss= 1.469449, Training Accuracy= 0.39062\n",
            "Iter 22, Loss= 1.467928, Training Accuracy= 0.40625\n",
            "Iter 22, Loss= 1.455342, Training Accuracy= 0.45312\n",
            "Iter 22, Loss= 1.484830, Training Accuracy= 0.32812\n",
            "Iter 22, Loss= 1.497551, Training Accuracy= 0.29688\n",
            "Iter 22, Loss= 1.482156, Training Accuracy= 0.37500\n",
            "Iter 22, Loss= 1.475197, Training Accuracy= 0.35938\n",
            "Iter 22, Loss= 1.473278, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 23, Loss= 1.441555, Training Accuracy= 0.46875\n",
            "Iter 23, Loss= 1.444170, Training Accuracy= 0.46875\n",
            "Iter 23, Loss= 1.447640, Training Accuracy= 0.45312\n",
            "Iter 23, Loss= 1.487276, Training Accuracy= 0.34375\n",
            "Iter 23, Loss= 1.457813, Training Accuracy= 0.42188\n",
            "Iter 23, Loss= 1.455037, Training Accuracy= 0.40625\n",
            "Iter 23, Loss= 1.460258, Training Accuracy= 0.39062\n",
            "Iter 23, Loss= 1.459301, Training Accuracy= 0.40625\n",
            "Iter 23, Loss= 1.446391, Training Accuracy= 0.45312\n",
            "Iter 23, Loss= 1.477339, Training Accuracy= 0.32812\n",
            "Iter 23, Loss= 1.489650, Training Accuracy= 0.29688\n",
            "Iter 23, Loss= 1.475122, Training Accuracy= 0.37500\n",
            "Iter 23, Loss= 1.466985, Training Accuracy= 0.35938\n",
            "Iter 23, Loss= 1.465909, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 24, Loss= 1.432314, Training Accuracy= 0.46875\n",
            "Iter 24, Loss= 1.435663, Training Accuracy= 0.46875\n",
            "Iter 24, Loss= 1.439377, Training Accuracy= 0.45312\n",
            "Iter 24, Loss= 1.480791, Training Accuracy= 0.34375\n",
            "Iter 24, Loss= 1.449656, Training Accuracy= 0.42188\n",
            "Iter 24, Loss= 1.446740, Training Accuracy= 0.40625\n",
            "Iter 24, Loss= 1.451530, Training Accuracy= 0.39062\n",
            "Iter 24, Loss= 1.451092, Training Accuracy= 0.40625\n",
            "Iter 24, Loss= 1.437900, Training Accuracy= 0.45312\n",
            "Iter 24, Loss= 1.470378, Training Accuracy= 0.32812\n",
            "Iter 24, Loss= 1.482050, Training Accuracy= 0.29688\n",
            "Iter 24, Loss= 1.468352, Training Accuracy= 0.37500\n",
            "Iter 24, Loss= 1.459173, Training Accuracy= 0.35938\n",
            "Iter 24, Loss= 1.459009, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 25, Loss= 1.423598, Training Accuracy= 0.46875\n",
            "Iter 25, Loss= 1.427596, Training Accuracy= 0.46875\n",
            "Iter 25, Loss= 1.431555, Training Accuracy= 0.45312\n",
            "Iter 25, Loss= 1.474469, Training Accuracy= 0.34375\n",
            "Iter 25, Loss= 1.442296, Training Accuracy= 0.42188\n",
            "Iter 25, Loss= 1.439066, Training Accuracy= 0.40625\n",
            "Iter 25, Loss= 1.443404, Training Accuracy= 0.39062\n",
            "Iter 25, Loss= 1.443411, Training Accuracy= 0.40625\n",
            "Iter 25, Loss= 1.430179, Training Accuracy= 0.45312\n",
            "Iter 25, Loss= 1.464073, Training Accuracy= 0.32812\n",
            "Iter 25, Loss= 1.474807, Training Accuracy= 0.29688\n",
            "Iter 25, Loss= 1.461851, Training Accuracy= 0.37500\n",
            "Iter 25, Loss= 1.451863, Training Accuracy= 0.35938\n",
            "Iter 25, Loss= 1.452454, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 26, Loss= 1.415751, Training Accuracy= 0.46875\n",
            "Iter 26, Loss= 1.420264, Training Accuracy= 0.46875\n",
            "Iter 26, Loss= 1.424616, Training Accuracy= 0.45312\n",
            "Iter 26, Loss= 1.468581, Training Accuracy= 0.34375\n",
            "Iter 26, Loss= 1.435969, Training Accuracy= 0.42188\n",
            "Iter 26, Loss= 1.432537, Training Accuracy= 0.40625\n",
            "Iter 26, Loss= 1.436149, Training Accuracy= 0.39062\n",
            "Iter 26, Loss= 1.436890, Training Accuracy= 0.40625\n",
            "Iter 26, Loss= 1.423655, Training Accuracy= 0.45312\n",
            "Iter 26, Loss= 1.458244, Training Accuracy= 0.32812\n",
            "Iter 26, Loss= 1.468250, Training Accuracy= 0.29688\n",
            "Iter 26, Loss= 1.455840, Training Accuracy= 0.37500\n",
            "Iter 26, Loss= 1.445405, Training Accuracy= 0.35938\n",
            "Iter 26, Loss= 1.446477, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 27, Loss= 1.408956, Training Accuracy= 0.46875\n",
            "Iter 27, Loss= 1.414001, Training Accuracy= 0.46875\n",
            "Iter 27, Loss= 1.418640, Training Accuracy= 0.45312\n",
            "Iter 27, Loss= 1.463351, Training Accuracy= 0.34375\n",
            "Iter 27, Loss= 1.430452, Training Accuracy= 0.42188\n",
            "Iter 27, Loss= 1.426780, Training Accuracy= 0.40625\n",
            "Iter 27, Loss= 1.430018, Training Accuracy= 0.39062\n",
            "Iter 27, Loss= 1.431297, Training Accuracy= 0.40625\n",
            "Iter 27, Loss= 1.418147, Training Accuracy= 0.45312\n",
            "Iter 27, Loss= 1.452960, Training Accuracy= 0.32812\n",
            "Iter 27, Loss= 1.462531, Training Accuracy= 0.29688\n",
            "Iter 27, Loss= 1.450601, Training Accuracy= 0.37500\n",
            "Iter 27, Loss= 1.439893, Training Accuracy= 0.35938\n",
            "Iter 27, Loss= 1.441410, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 28, Loss= 1.403228, Training Accuracy= 0.46875\n",
            "Iter 28, Loss= 1.408659, Training Accuracy= 0.46875\n",
            "Iter 28, Loss= 1.413453, Training Accuracy= 0.45312\n",
            "Iter 28, Loss= 1.458436, Training Accuracy= 0.34375\n",
            "Iter 28, Loss= 1.425474, Training Accuracy= 0.42188\n",
            "Iter 28, Loss= 1.421662, Training Accuracy= 0.40625\n",
            "Iter 28, Loss= 1.424622, Training Accuracy= 0.39062\n",
            "Iter 28, Loss= 1.426301, Training Accuracy= 0.40625\n",
            "Iter 28, Loss= 1.413330, Training Accuracy= 0.45312\n",
            "Iter 28, Loss= 1.448256, Training Accuracy= 0.32812\n",
            "Iter 28, Loss= 1.457287, Training Accuracy= 0.29688\n",
            "Iter 28, Loss= 1.445620, Training Accuracy= 0.37500\n",
            "Iter 28, Loss= 1.434829, Training Accuracy= 0.35938\n",
            "Iter 28, Loss= 1.436722, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 29, Loss= 1.398221, Training Accuracy= 0.46875\n",
            "Iter 29, Loss= 1.404045, Training Accuracy= 0.46875\n",
            "Iter 29, Loss= 1.408629, Training Accuracy= 0.45312\n",
            "Iter 29, Loss= 1.453768, Training Accuracy= 0.34375\n",
            "Iter 29, Loss= 1.420916, Training Accuracy= 0.42188\n",
            "Iter 29, Loss= 1.416975, Training Accuracy= 0.40625\n",
            "Iter 29, Loss= 1.419790, Training Accuracy= 0.39062\n",
            "Iter 29, Loss= 1.421709, Training Accuracy= 0.40625\n",
            "Iter 29, Loss= 1.408925, Training Accuracy= 0.45312\n",
            "Iter 29, Loss= 1.443739, Training Accuracy= 0.32812\n",
            "Iter 29, Loss= 1.452384, Training Accuracy= 0.29688\n",
            "Iter 29, Loss= 1.440871, Training Accuracy= 0.37500\n",
            "Iter 29, Loss= 1.430083, Training Accuracy= 0.35938\n",
            "Iter 29, Loss= 1.432319, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 30, Loss= 1.393819, Training Accuracy= 0.46875\n",
            "Iter 30, Loss= 1.399744, Training Accuracy= 0.46875\n",
            "Iter 30, Loss= 1.404098, Training Accuracy= 0.45312\n",
            "Iter 30, Loss= 1.449336, Training Accuracy= 0.34375\n",
            "Iter 30, Loss= 1.416684, Training Accuracy= 0.42188\n",
            "Iter 30, Loss= 1.412565, Training Accuracy= 0.40625\n",
            "Iter 30, Loss= 1.415442, Training Accuracy= 0.39062\n",
            "Iter 30, Loss= 1.417496, Training Accuracy= 0.40625\n",
            "Iter 30, Loss= 1.404715, Training Accuracy= 0.45312\n",
            "Iter 30, Loss= 1.439351, Training Accuracy= 0.32812\n",
            "Iter 30, Loss= 1.447826, Training Accuracy= 0.29688\n",
            "Iter 30, Loss= 1.436318, Training Accuracy= 0.37500\n",
            "Iter 30, Loss= 1.425658, Training Accuracy= 0.35938\n",
            "Iter 30, Loss= 1.428052, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 31, Loss= 1.389816, Training Accuracy= 0.46875\n",
            "Iter 31, Loss= 1.395678, Training Accuracy= 0.46875\n",
            "Iter 31, Loss= 1.399822, Training Accuracy= 0.45312\n",
            "Iter 31, Loss= 1.445031, Training Accuracy= 0.34375\n",
            "Iter 31, Loss= 1.412606, Training Accuracy= 0.42188\n",
            "Iter 31, Loss= 1.408330, Training Accuracy= 0.40625\n",
            "Iter 31, Loss= 1.411273, Training Accuracy= 0.39062\n",
            "Iter 31, Loss= 1.413492, Training Accuracy= 0.40625\n",
            "Iter 31, Loss= 1.400677, Training Accuracy= 0.45312\n",
            "Iter 31, Loss= 1.435066, Training Accuracy= 0.32812\n",
            "Iter 31, Loss= 1.443440, Training Accuracy= 0.29688\n",
            "Iter 31, Loss= 1.431977, Training Accuracy= 0.37500\n",
            "Iter 31, Loss= 1.421378, Training Accuracy= 0.35938\n",
            "Iter 31, Loss= 1.423889, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 32, Loss= 1.385958, Training Accuracy= 0.46875\n",
            "Iter 32, Loss= 1.391715, Training Accuracy= 0.46875\n",
            "Iter 32, Loss= 1.395782, Training Accuracy= 0.45312\n",
            "Iter 32, Loss= 1.440794, Training Accuracy= 0.34375\n",
            "Iter 32, Loss= 1.408630, Training Accuracy= 0.42188\n",
            "Iter 32, Loss= 1.404234, Training Accuracy= 0.40625\n",
            "Iter 32, Loss= 1.407209, Training Accuracy= 0.39062\n",
            "Iter 32, Loss= 1.409573, Training Accuracy= 0.40625\n",
            "Iter 32, Loss= 1.396765, Training Accuracy= 0.45312\n",
            "Iter 32, Loss= 1.430944, Training Accuracy= 0.32812\n",
            "Iter 32, Loss= 1.439129, Training Accuracy= 0.29688\n",
            "Iter 32, Loss= 1.427792, Training Accuracy= 0.37500\n",
            "Iter 32, Loss= 1.417191, Training Accuracy= 0.35938\n",
            "Iter 32, Loss= 1.419825, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 33, Loss= 1.382187, Training Accuracy= 0.46875\n",
            "Iter 33, Loss= 1.387842, Training Accuracy= 0.46875\n",
            "Iter 33, Loss= 1.391863, Training Accuracy= 0.45312\n",
            "Iter 33, Loss= 1.436615, Training Accuracy= 0.34375\n",
            "Iter 33, Loss= 1.404706, Training Accuracy= 0.42188\n",
            "Iter 33, Loss= 1.400222, Training Accuracy= 0.40625\n",
            "Iter 33, Loss= 1.403212, Training Accuracy= 0.39062\n",
            "Iter 33, Loss= 1.405710, Training Accuracy= 0.40625\n",
            "Iter 33, Loss= 1.392915, Training Accuracy= 0.45312\n",
            "Iter 33, Loss= 1.426891, Training Accuracy= 0.32812\n",
            "Iter 33, Loss= 1.434879, Training Accuracy= 0.29688\n",
            "Iter 33, Loss= 1.423667, Training Accuracy= 0.37500\n",
            "Iter 33, Loss= 1.413087, Training Accuracy= 0.35938\n",
            "Iter 33, Loss= 1.415810, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 34, Loss= 1.378481, Training Accuracy= 0.46875\n",
            "Iter 34, Loss= 1.384058, Training Accuracy= 0.46875\n",
            "Iter 34, Loss= 1.388036, Training Accuracy= 0.45312\n",
            "Iter 34, Loss= 1.432518, Training Accuracy= 0.34375\n",
            "Iter 34, Loss= 1.400817, Training Accuracy= 0.42188\n",
            "Iter 34, Loss= 1.396293, Training Accuracy= 0.40625\n",
            "Iter 34, Loss= 1.399305, Training Accuracy= 0.39062\n",
            "Iter 34, Loss= 1.401904, Training Accuracy= 0.40625\n",
            "Iter 34, Loss= 1.389123, Training Accuracy= 0.45312\n",
            "Iter 34, Loss= 1.422883, Training Accuracy= 0.32812\n",
            "Iter 34, Loss= 1.430692, Training Accuracy= 0.29688\n",
            "Iter 34, Loss= 1.419610, Training Accuracy= 0.37500\n",
            "Iter 34, Loss= 1.409102, Training Accuracy= 0.35938\n",
            "Iter 34, Loss= 1.411841, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 35, Loss= 1.374844, Training Accuracy= 0.46875\n",
            "Iter 35, Loss= 1.380368, Training Accuracy= 0.46875\n",
            "Iter 35, Loss= 1.384299, Training Accuracy= 0.45312\n",
            "Iter 35, Loss= 1.428497, Training Accuracy= 0.34375\n",
            "Iter 35, Loss= 1.396983, Training Accuracy= 0.42188\n",
            "Iter 35, Loss= 1.392429, Training Accuracy= 0.40625\n",
            "Iter 35, Loss= 1.395475, Training Accuracy= 0.39062\n",
            "Iter 35, Loss= 1.398148, Training Accuracy= 0.40625\n",
            "Iter 35, Loss= 1.385381, Training Accuracy= 0.45312\n",
            "Iter 35, Loss= 1.418905, Training Accuracy= 0.32812\n",
            "Iter 35, Loss= 1.426578, Training Accuracy= 0.29688\n",
            "Iter 35, Loss= 1.415625, Training Accuracy= 0.37500\n",
            "Iter 35, Loss= 1.405175, Training Accuracy= 0.35938\n",
            "Iter 35, Loss= 1.407918, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 36, Loss= 1.371267, Training Accuracy= 0.46875\n",
            "Iter 36, Loss= 1.376733, Training Accuracy= 0.46875\n",
            "Iter 36, Loss= 1.380624, Training Accuracy= 0.45312\n",
            "Iter 36, Loss= 1.424530, Training Accuracy= 0.34375\n",
            "Iter 36, Loss= 1.393183, Training Accuracy= 0.42188\n",
            "Iter 36, Loss= 1.388690, Training Accuracy= 0.40625\n",
            "Iter 36, Loss= 1.391689, Training Accuracy= 0.39062\n",
            "Iter 36, Loss= 1.394426, Training Accuracy= 0.40625\n",
            "Iter 36, Loss= 1.381683, Training Accuracy= 0.45312\n",
            "Iter 36, Loss= 1.414955, Training Accuracy= 0.32812\n",
            "Iter 36, Loss= 1.422513, Training Accuracy= 0.29688\n",
            "Iter 36, Loss= 1.411699, Training Accuracy= 0.37500\n",
            "Iter 36, Loss= 1.401297, Training Accuracy= 0.35938\n",
            "Iter 36, Loss= 1.404044, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 37, Loss= 1.367747, Training Accuracy= 0.46875\n",
            "Iter 37, Loss= 1.373166, Training Accuracy= 0.46875\n",
            "Iter 37, Loss= 1.377011, Training Accuracy= 0.45312\n",
            "Iter 37, Loss= 1.420605, Training Accuracy= 0.34375\n",
            "Iter 37, Loss= 1.389419, Training Accuracy= 0.42188\n",
            "Iter 37, Loss= 1.385005, Training Accuracy= 0.40625\n",
            "Iter 37, Loss= 1.387943, Training Accuracy= 0.39062\n",
            "Iter 37, Loss= 1.390737, Training Accuracy= 0.40625\n",
            "Iter 37, Loss= 1.378038, Training Accuracy= 0.45312\n",
            "Iter 37, Loss= 1.411039, Training Accuracy= 0.32812\n",
            "Iter 37, Loss= 1.418483, Training Accuracy= 0.29688\n",
            "Iter 37, Loss= 1.407809, Training Accuracy= 0.37500\n",
            "Iter 37, Loss= 1.397467, Training Accuracy= 0.35938\n",
            "Iter 37, Loss= 1.400209, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 38, Loss= 1.364274, Training Accuracy= 0.46875\n",
            "Iter 38, Loss= 1.369651, Training Accuracy= 0.46875\n",
            "Iter 38, Loss= 1.373482, Training Accuracy= 0.45312\n",
            "Iter 38, Loss= 1.416710, Training Accuracy= 0.34375\n",
            "Iter 38, Loss= 1.385678, Training Accuracy= 0.42188\n",
            "Iter 38, Loss= 1.381364, Training Accuracy= 0.40625\n",
            "Iter 38, Loss= 1.384233, Training Accuracy= 0.39062\n",
            "Iter 38, Loss= 1.387079, Training Accuracy= 0.40625\n",
            "Iter 38, Loss= 1.374426, Training Accuracy= 0.45312\n",
            "Iter 38, Loss= 1.407160, Training Accuracy= 0.32812\n",
            "Iter 38, Loss= 1.414487, Training Accuracy= 0.29688\n",
            "Iter 38, Loss= 1.403964, Training Accuracy= 0.37500\n",
            "Iter 38, Loss= 1.393693, Training Accuracy= 0.35938\n",
            "Iter 38, Loss= 1.396415, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 39, Loss= 1.360840, Training Accuracy= 0.46875\n",
            "Iter 39, Loss= 1.366181, Training Accuracy= 0.46875\n",
            "Iter 39, Loss= 1.370023, Training Accuracy= 0.45312\n",
            "Iter 39, Loss= 1.412848, Training Accuracy= 0.34375\n",
            "Iter 39, Loss= 1.381950, Training Accuracy= 0.42188\n",
            "Iter 39, Loss= 1.377773, Training Accuracy= 0.40625\n",
            "Iter 39, Loss= 1.380559, Training Accuracy= 0.39062\n",
            "Iter 39, Loss= 1.383442, Training Accuracy= 0.40625\n",
            "Iter 39, Loss= 1.370847, Training Accuracy= 0.45312\n",
            "Iter 39, Loss= 1.403311, Training Accuracy= 0.32812\n",
            "Iter 39, Loss= 1.410512, Training Accuracy= 0.29688\n",
            "Iter 39, Loss= 1.400155, Training Accuracy= 0.37500\n",
            "Iter 39, Loss= 1.389947, Training Accuracy= 0.35938\n",
            "Iter 39, Loss= 1.392664, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 40, Loss= 1.357439, Training Accuracy= 0.46875\n",
            "Iter 40, Loss= 1.362751, Training Accuracy= 0.46875\n",
            "Iter 40, Loss= 1.366602, Training Accuracy= 0.45312\n",
            "Iter 40, Loss= 1.408995, Training Accuracy= 0.34375\n",
            "Iter 40, Loss= 1.378242, Training Accuracy= 0.42188\n",
            "Iter 40, Loss= 1.374218, Training Accuracy= 0.40625\n",
            "Iter 40, Loss= 1.376922, Training Accuracy= 0.39062\n",
            "Iter 40, Loss= 1.379823, Training Accuracy= 0.40625\n",
            "Iter 40, Loss= 1.367295, Training Accuracy= 0.45312\n",
            "Iter 40, Loss= 1.399486, Training Accuracy= 0.32812\n",
            "Iter 40, Loss= 1.406558, Training Accuracy= 0.29688\n",
            "Iter 40, Loss= 1.396385, Training Accuracy= 0.37500\n",
            "Iter 40, Loss= 1.386227, Training Accuracy= 0.35938\n",
            "Iter 40, Loss= 1.388940, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 41, Loss= 1.354067, Training Accuracy= 0.46875\n",
            "Iter 41, Loss= 1.359355, Training Accuracy= 0.46875\n",
            "Iter 41, Loss= 1.363214, Training Accuracy= 0.45312\n",
            "Iter 41, Loss= 1.405158, Training Accuracy= 0.34375\n",
            "Iter 41, Loss= 1.374553, Training Accuracy= 0.42188\n",
            "Iter 41, Loss= 1.370690, Training Accuracy= 0.40625\n",
            "Iter 41, Loss= 1.373310, Training Accuracy= 0.39062\n",
            "Iter 41, Loss= 1.376219, Training Accuracy= 0.40625\n",
            "Iter 41, Loss= 1.363767, Training Accuracy= 0.45312\n",
            "Iter 41, Loss= 1.395686, Training Accuracy= 0.32812\n",
            "Iter 41, Loss= 1.402620, Training Accuracy= 0.29688\n",
            "Iter 41, Loss= 1.392643, Training Accuracy= 0.37500\n",
            "Iter 41, Loss= 1.382528, Training Accuracy= 0.35938\n",
            "Iter 41, Loss= 1.385242, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 42, Loss= 1.350722, Training Accuracy= 0.46875\n",
            "Iter 42, Loss= 1.355989, Training Accuracy= 0.46875\n",
            "Iter 42, Loss= 1.359855, Training Accuracy= 0.45312\n",
            "Iter 42, Loss= 1.401340, Training Accuracy= 0.34375\n",
            "Iter 42, Loss= 1.370876, Training Accuracy= 0.42188\n",
            "Iter 42, Loss= 1.367185, Training Accuracy= 0.40625\n",
            "Iter 42, Loss= 1.369728, Training Accuracy= 0.39062\n",
            "Iter 42, Loss= 1.372627, Training Accuracy= 0.40625\n",
            "Iter 42, Loss= 1.360257, Training Accuracy= 0.45312\n",
            "Iter 42, Loss= 1.391907, Training Accuracy= 0.32812\n",
            "Iter 42, Loss= 1.398697, Training Accuracy= 0.29688\n",
            "Iter 42, Loss= 1.388926, Training Accuracy= 0.37500\n",
            "Iter 42, Loss= 1.378847, Training Accuracy= 0.35938\n",
            "Iter 42, Loss= 1.381568, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 43, Loss= 1.347397, Training Accuracy= 0.46875\n",
            "Iter 43, Loss= 1.352648, Training Accuracy= 0.46875\n",
            "Iter 43, Loss= 1.356518, Training Accuracy= 0.45312\n",
            "Iter 43, Loss= 1.397537, Training Accuracy= 0.34375\n",
            "Iter 43, Loss= 1.367210, Training Accuracy= 0.42188\n",
            "Iter 43, Loss= 1.363703, Training Accuracy= 0.40625\n",
            "Iter 43, Loss= 1.366168, Training Accuracy= 0.39062\n",
            "Iter 43, Loss= 1.369041, Training Accuracy= 0.40625\n",
            "Iter 43, Loss= 1.356758, Training Accuracy= 0.45312\n",
            "Iter 43, Loss= 1.388148, Training Accuracy= 0.32812\n",
            "Iter 43, Loss= 1.394788, Training Accuracy= 0.29688\n",
            "Iter 43, Loss= 1.385227, Training Accuracy= 0.37500\n",
            "Iter 43, Loss= 1.375185, Training Accuracy= 0.35938\n",
            "Iter 43, Loss= 1.377912, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 44, Loss= 1.344095, Training Accuracy= 0.46875\n",
            "Iter 44, Loss= 1.349331, Training Accuracy= 0.46875\n",
            "Iter 44, Loss= 1.353202, Training Accuracy= 0.45312\n",
            "Iter 44, Loss= 1.393755, Training Accuracy= 0.34375\n",
            "Iter 44, Loss= 1.363552, Training Accuracy= 0.42188\n",
            "Iter 44, Loss= 1.360250, Training Accuracy= 0.40625\n",
            "Iter 44, Loss= 1.362626, Training Accuracy= 0.39062\n",
            "Iter 44, Loss= 1.365468, Training Accuracy= 0.40625\n",
            "Iter 44, Loss= 1.353268, Training Accuracy= 0.45312\n",
            "Iter 44, Loss= 1.384405, Training Accuracy= 0.32812\n",
            "Iter 44, Loss= 1.390892, Training Accuracy= 0.29688\n",
            "Iter 44, Loss= 1.381548, Training Accuracy= 0.37500\n",
            "Iter 44, Loss= 1.371544, Training Accuracy= 0.35938\n",
            "Iter 44, Loss= 1.374276, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 45, Loss= 1.340813, Training Accuracy= 0.46875\n",
            "Iter 45, Loss= 1.346041, Training Accuracy= 0.46875\n",
            "Iter 45, Loss= 1.349906, Training Accuracy= 0.45312\n",
            "Iter 45, Loss= 1.389983, Training Accuracy= 0.34375\n",
            "Iter 45, Loss= 1.359900, Training Accuracy= 0.42188\n",
            "Iter 45, Loss= 1.356824, Training Accuracy= 0.40625\n",
            "Iter 45, Loss= 1.359104, Training Accuracy= 0.39062\n",
            "Iter 45, Loss= 1.361906, Training Accuracy= 0.40625\n",
            "Iter 45, Loss= 1.349790, Training Accuracy= 0.45312\n",
            "Iter 45, Loss= 1.380679, Training Accuracy= 0.32812\n",
            "Iter 45, Loss= 1.387010, Training Accuracy= 0.29688\n",
            "Iter 45, Loss= 1.377886, Training Accuracy= 0.37500\n",
            "Iter 45, Loss= 1.367921, Training Accuracy= 0.35938\n",
            "Iter 45, Loss= 1.370656, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 46, Loss= 1.337548, Training Accuracy= 0.46875\n",
            "Iter 46, Loss= 1.342765, Training Accuracy= 0.46875\n",
            "Iter 46, Loss= 1.346627, Training Accuracy= 0.45312\n",
            "Iter 46, Loss= 1.386219, Training Accuracy= 0.34375\n",
            "Iter 46, Loss= 1.356251, Training Accuracy= 0.42188\n",
            "Iter 46, Loss= 1.353419, Training Accuracy= 0.40625\n",
            "Iter 46, Loss= 1.355602, Training Accuracy= 0.39062\n",
            "Iter 46, Loss= 1.358354, Training Accuracy= 0.40625\n",
            "Iter 46, Loss= 1.346322, Training Accuracy= 0.45312\n",
            "Iter 46, Loss= 1.376969, Training Accuracy= 0.32812\n",
            "Iter 46, Loss= 1.383148, Training Accuracy= 0.29688\n",
            "Iter 46, Loss= 1.374238, Training Accuracy= 0.37500\n",
            "Iter 46, Loss= 1.364316, Training Accuracy= 0.35938\n",
            "Iter 46, Loss= 1.367051, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 47, Loss= 1.334296, Training Accuracy= 0.46875\n",
            "Iter 47, Loss= 1.339504, Training Accuracy= 0.46875\n",
            "Iter 47, Loss= 1.343368, Training Accuracy= 0.45312\n",
            "Iter 47, Loss= 1.382461, Training Accuracy= 0.34375\n",
            "Iter 47, Loss= 1.352612, Training Accuracy= 0.42188\n",
            "Iter 47, Loss= 1.350030, Training Accuracy= 0.40625\n",
            "Iter 47, Loss= 1.352118, Training Accuracy= 0.39062\n",
            "Iter 47, Loss= 1.354810, Training Accuracy= 0.40625\n",
            "Iter 47, Loss= 1.342859, Training Accuracy= 0.45312\n",
            "Iter 47, Loss= 1.373280, Training Accuracy= 0.32812\n",
            "Iter 47, Loss= 1.379303, Training Accuracy= 0.29688\n",
            "Iter 47, Loss= 1.370601, Training Accuracy= 0.37500\n",
            "Iter 47, Loss= 1.360727, Training Accuracy= 0.35938\n",
            "Iter 47, Loss= 1.363459, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 48, Loss= 1.331057, Training Accuracy= 0.46875\n",
            "Iter 48, Loss= 1.336258, Training Accuracy= 0.46875\n",
            "Iter 48, Loss= 1.340119, Training Accuracy= 0.45312\n",
            "Iter 48, Loss= 1.378713, Training Accuracy= 0.34375\n",
            "Iter 48, Loss= 1.348979, Training Accuracy= 0.42188\n",
            "Iter 48, Loss= 1.346657, Training Accuracy= 0.40625\n",
            "Iter 48, Loss= 1.348650, Training Accuracy= 0.39062\n",
            "Iter 48, Loss= 1.351276, Training Accuracy= 0.40625\n",
            "Iter 48, Loss= 1.339401, Training Accuracy= 0.45312\n",
            "Iter 48, Loss= 1.369605, Training Accuracy= 0.32812\n",
            "Iter 48, Loss= 1.375472, Training Accuracy= 0.29688\n",
            "Iter 48, Loss= 1.366976, Training Accuracy= 0.37500\n",
            "Iter 48, Loss= 1.357155, Training Accuracy= 0.35938\n",
            "Iter 48, Loss= 1.359878, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 49, Loss= 1.327828, Training Accuracy= 0.46875\n",
            "Iter 49, Loss= 1.333021, Training Accuracy= 0.46875\n",
            "Iter 49, Loss= 1.336877, Training Accuracy= 0.45312\n",
            "Iter 49, Loss= 1.374967, Training Accuracy= 0.34375\n",
            "Iter 49, Loss= 1.345348, Training Accuracy= 0.42188\n",
            "Iter 49, Loss= 1.343298, Training Accuracy= 0.40625\n",
            "Iter 49, Loss= 1.345197, Training Accuracy= 0.39062\n",
            "Iter 49, Loss= 1.347755, Training Accuracy= 0.40625\n",
            "Iter 49, Loss= 1.335946, Training Accuracy= 0.45312\n",
            "Iter 49, Loss= 1.365943, Training Accuracy= 0.32812\n",
            "Iter 49, Loss= 1.371651, Training Accuracy= 0.29688\n",
            "Iter 49, Loss= 1.363362, Training Accuracy= 0.37500\n",
            "Iter 49, Loss= 1.353611, Training Accuracy= 0.35938\n",
            "Iter 49, Loss= 1.356308, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 50, Loss= 1.324605, Training Accuracy= 0.46875\n",
            "Iter 50, Loss= 1.329792, Training Accuracy= 0.46875\n",
            "Iter 50, Loss= 1.333640, Training Accuracy= 0.45312\n",
            "Iter 50, Loss= 1.371229, Training Accuracy= 0.34375\n",
            "Iter 50, Loss= 1.341717, Training Accuracy= 0.42188\n",
            "Iter 50, Loss= 1.339948, Training Accuracy= 0.40625\n",
            "Iter 50, Loss= 1.341757, Training Accuracy= 0.39062\n",
            "Iter 50, Loss= 1.344238, Training Accuracy= 0.40625\n",
            "Iter 50, Loss= 1.332494, Training Accuracy= 0.45312\n",
            "Iter 50, Loss= 1.362295, Training Accuracy= 0.32812\n",
            "Iter 50, Loss= 1.367843, Training Accuracy= 0.29688\n",
            "Iter 50, Loss= 1.359750, Training Accuracy= 0.37500\n",
            "Iter 50, Loss= 1.350077, Training Accuracy= 0.35938\n",
            "Iter 50, Loss= 1.352750, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 51, Loss= 1.321390, Training Accuracy= 0.46875\n",
            "Iter 51, Loss= 1.326568, Training Accuracy= 0.46875\n",
            "Iter 51, Loss= 1.330406, Training Accuracy= 0.45312\n",
            "Iter 51, Loss= 1.367490, Training Accuracy= 0.34375\n",
            "Iter 51, Loss= 1.338084, Training Accuracy= 0.42188\n",
            "Iter 51, Loss= 1.336612, Training Accuracy= 0.40625\n",
            "Iter 51, Loss= 1.338326, Training Accuracy= 0.39062\n",
            "Iter 51, Loss= 1.340726, Training Accuracy= 0.40625\n",
            "Iter 51, Loss= 1.329042, Training Accuracy= 0.45312\n",
            "Iter 51, Loss= 1.358658, Training Accuracy= 0.32812\n",
            "Iter 51, Loss= 1.364044, Training Accuracy= 0.29688\n",
            "Iter 51, Loss= 1.356136, Training Accuracy= 0.37500\n",
            "Iter 51, Loss= 1.346551, Training Accuracy= 0.35938\n",
            "Iter 51, Loss= 1.349200, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 52, Loss= 1.318178, Training Accuracy= 0.46875\n",
            "Iter 52, Loss= 1.323344, Training Accuracy= 0.46875\n",
            "Iter 52, Loss= 1.327173, Training Accuracy= 0.45312\n",
            "Iter 52, Loss= 1.363750, Training Accuracy= 0.34375\n",
            "Iter 52, Loss= 1.334452, Training Accuracy= 0.42188\n",
            "Iter 52, Loss= 1.333284, Training Accuracy= 0.40625\n",
            "Iter 52, Loss= 1.334902, Training Accuracy= 0.39062\n",
            "Iter 52, Loss= 1.337215, Training Accuracy= 0.40625\n",
            "Iter 52, Loss= 1.325590, Training Accuracy= 0.45312\n",
            "Iter 52, Loss= 1.355028, Training Accuracy= 0.32812\n",
            "Iter 52, Loss= 1.360256, Training Accuracy= 0.29688\n",
            "Iter 52, Loss= 1.352521, Training Accuracy= 0.37500\n",
            "Iter 52, Loss= 1.343033, Training Accuracy= 0.35938\n",
            "Iter 52, Loss= 1.345653, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 53, Loss= 1.314967, Training Accuracy= 0.46875\n",
            "Iter 53, Loss= 1.320119, Training Accuracy= 0.46875\n",
            "Iter 53, Loss= 1.323939, Training Accuracy= 0.45312\n",
            "Iter 53, Loss= 1.360012, Training Accuracy= 0.34375\n",
            "Iter 53, Loss= 1.330823, Training Accuracy= 0.42188\n",
            "Iter 53, Loss= 1.329962, Training Accuracy= 0.40625\n",
            "Iter 53, Loss= 1.331485, Training Accuracy= 0.39062\n",
            "Iter 53, Loss= 1.333706, Training Accuracy= 0.40625\n",
            "Iter 53, Loss= 1.322138, Training Accuracy= 0.45312\n",
            "Iter 53, Loss= 1.351406, Training Accuracy= 0.32812\n",
            "Iter 53, Loss= 1.356478, Training Accuracy= 0.29688\n",
            "Iter 53, Loss= 1.348903, Training Accuracy= 0.37500\n",
            "Iter 53, Loss= 1.339525, Training Accuracy= 0.35938\n",
            "Iter 53, Loss= 1.342109, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 54, Loss= 1.311759, Training Accuracy= 0.46875\n",
            "Iter 54, Loss= 1.316889, Training Accuracy= 0.46875\n",
            "Iter 54, Loss= 1.320701, Training Accuracy= 0.45312\n",
            "Iter 54, Loss= 1.356269, Training Accuracy= 0.34375\n",
            "Iter 54, Loss= 1.327197, Training Accuracy= 0.42188\n",
            "Iter 54, Loss= 1.326645, Training Accuracy= 0.40625\n",
            "Iter 54, Loss= 1.328084, Training Accuracy= 0.39062\n",
            "Iter 54, Loss= 1.330202, Training Accuracy= 0.40625\n",
            "Iter 54, Loss= 1.318683, Training Accuracy= 0.45312\n",
            "Iter 54, Loss= 1.347790, Training Accuracy= 0.32812\n",
            "Iter 54, Loss= 1.352717, Training Accuracy= 0.29688\n",
            "Iter 54, Loss= 1.345282, Training Accuracy= 0.37500\n",
            "Iter 54, Loss= 1.336029, Training Accuracy= 0.35938\n",
            "Iter 54, Loss= 1.338567, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 55, Loss= 1.308550, Training Accuracy= 0.46875\n",
            "Iter 55, Loss= 1.313653, Training Accuracy= 0.46875\n",
            "Iter 55, Loss= 1.317455, Training Accuracy= 0.45312\n",
            "Iter 55, Loss= 1.352516, Training Accuracy= 0.34375\n",
            "Iter 55, Loss= 1.323573, Training Accuracy= 0.42188\n",
            "Iter 55, Loss= 1.323331, Training Accuracy= 0.40625\n",
            "Iter 55, Loss= 1.324692, Training Accuracy= 0.39062\n",
            "Iter 55, Loss= 1.326711, Training Accuracy= 0.40625\n",
            "Iter 55, Loss= 1.315228, Training Accuracy= 0.45312\n",
            "Iter 55, Loss= 1.344178, Training Accuracy= 0.32812\n",
            "Iter 55, Loss= 1.348968, Training Accuracy= 0.29688\n",
            "Iter 55, Loss= 1.341657, Training Accuracy= 0.37500\n",
            "Iter 55, Loss= 1.332542, Training Accuracy= 0.35938\n",
            "Iter 55, Loss= 1.335026, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 56, Loss= 1.305339, Training Accuracy= 0.46875\n",
            "Iter 56, Loss= 1.310411, Training Accuracy= 0.46875\n",
            "Iter 56, Loss= 1.314200, Training Accuracy= 0.45312\n",
            "Iter 56, Loss= 1.348753, Training Accuracy= 0.34375\n",
            "Iter 56, Loss= 1.319946, Training Accuracy= 0.42188\n",
            "Iter 56, Loss= 1.320020, Training Accuracy= 0.40625\n",
            "Iter 56, Loss= 1.321307, Training Accuracy= 0.39062\n",
            "Iter 56, Loss= 1.323222, Training Accuracy= 0.40625\n",
            "Iter 56, Loss= 1.311767, Training Accuracy= 0.45312\n",
            "Iter 56, Loss= 1.340570, Training Accuracy= 0.32812\n",
            "Iter 56, Loss= 1.345228, Training Accuracy= 0.29688\n",
            "Iter 56, Loss= 1.338025, Training Accuracy= 0.37500\n",
            "Iter 56, Loss= 1.329058, Training Accuracy= 0.35938\n",
            "Iter 56, Loss= 1.331483, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 57, Loss= 1.302121, Training Accuracy= 0.46875\n",
            "Iter 57, Loss= 1.307157, Training Accuracy= 0.46875\n",
            "Iter 57, Loss= 1.310935, Training Accuracy= 0.45312\n",
            "Iter 57, Loss= 1.344980, Training Accuracy= 0.34375\n",
            "Iter 57, Loss= 1.316313, Training Accuracy= 0.42188\n",
            "Iter 57, Loss= 1.316712, Training Accuracy= 0.40625\n",
            "Iter 57, Loss= 1.317929, Training Accuracy= 0.39062\n",
            "Iter 57, Loss= 1.319733, Training Accuracy= 0.40625\n",
            "Iter 57, Loss= 1.308299, Training Accuracy= 0.45312\n",
            "Iter 57, Loss= 1.336966, Training Accuracy= 0.32812\n",
            "Iter 57, Loss= 1.341497, Training Accuracy= 0.29688\n",
            "Iter 57, Loss= 1.334386, Training Accuracy= 0.37500\n",
            "Iter 57, Loss= 1.325567, Training Accuracy= 0.35938\n",
            "Iter 57, Loss= 1.327939, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 58, Loss= 1.298894, Training Accuracy= 0.46875\n",
            "Iter 58, Loss= 1.303891, Training Accuracy= 0.46875\n",
            "Iter 58, Loss= 1.307661, Training Accuracy= 0.45312\n",
            "Iter 58, Loss= 1.341195, Training Accuracy= 0.34375\n",
            "Iter 58, Loss= 1.312678, Training Accuracy= 0.42188\n",
            "Iter 58, Loss= 1.313406, Training Accuracy= 0.40625\n",
            "Iter 58, Loss= 1.314560, Training Accuracy= 0.39062\n",
            "Iter 58, Loss= 1.316245, Training Accuracy= 0.40625\n",
            "Iter 58, Loss= 1.304822, Training Accuracy= 0.45312\n",
            "Iter 58, Loss= 1.333365, Training Accuracy= 0.32812\n",
            "Iter 58, Loss= 1.337782, Training Accuracy= 0.29688\n",
            "Iter 58, Loss= 1.330739, Training Accuracy= 0.37500\n",
            "Iter 58, Loss= 1.322082, Training Accuracy= 0.35938\n",
            "Iter 58, Loss= 1.324392, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 59, Loss= 1.295659, Training Accuracy= 0.46875\n",
            "Iter 59, Loss= 1.300614, Training Accuracy= 0.46875\n",
            "Iter 59, Loss= 1.304375, Training Accuracy= 0.45312\n",
            "Iter 59, Loss= 1.337400, Training Accuracy= 0.34375\n",
            "Iter 59, Loss= 1.309042, Training Accuracy= 0.42188\n",
            "Iter 59, Loss= 1.310103, Training Accuracy= 0.40625\n",
            "Iter 59, Loss= 1.311200, Training Accuracy= 0.39062\n",
            "Iter 59, Loss= 1.312756, Training Accuracy= 0.40625\n",
            "Iter 59, Loss= 1.301339, Training Accuracy= 0.45312\n",
            "Iter 59, Loss= 1.329767, Training Accuracy= 0.32812\n",
            "Iter 59, Loss= 1.334080, Training Accuracy= 0.29688\n",
            "Iter 59, Loss= 1.327079, Training Accuracy= 0.37500\n",
            "Iter 59, Loss= 1.318603, Training Accuracy= 0.35938\n",
            "Iter 59, Loss= 1.320845, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 60, Loss= 1.292414, Training Accuracy= 0.46875\n",
            "Iter 60, Loss= 1.297318, Training Accuracy= 0.46875\n",
            "Iter 60, Loss= 1.301078, Training Accuracy= 0.45312\n",
            "Iter 60, Loss= 1.333594, Training Accuracy= 0.34375\n",
            "Iter 60, Loss= 1.305403, Training Accuracy= 0.42188\n",
            "Iter 60, Loss= 1.306800, Training Accuracy= 0.40625\n",
            "Iter 60, Loss= 1.307850, Training Accuracy= 0.39062\n",
            "Iter 60, Loss= 1.309269, Training Accuracy= 0.40625\n",
            "Iter 60, Loss= 1.297852, Training Accuracy= 0.45312\n",
            "Iter 60, Loss= 1.326172, Training Accuracy= 0.32812\n",
            "Iter 60, Loss= 1.330393, Training Accuracy= 0.29688\n",
            "Iter 60, Loss= 1.323410, Training Accuracy= 0.37500\n",
            "Iter 60, Loss= 1.315129, Training Accuracy= 0.35938\n",
            "Iter 60, Loss= 1.317299, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 61, Loss= 1.289161, Training Accuracy= 0.46875\n",
            "Iter 61, Loss= 1.294003, Training Accuracy= 0.46875\n",
            "Iter 61, Loss= 1.297769, Training Accuracy= 0.45312\n",
            "Iter 61, Loss= 1.329780, Training Accuracy= 0.34375\n",
            "Iter 61, Loss= 1.301762, Training Accuracy= 0.42188\n",
            "Iter 61, Loss= 1.303504, Training Accuracy= 0.40625\n",
            "Iter 61, Loss= 1.304515, Training Accuracy= 0.39062\n",
            "Iter 61, Loss= 1.305787, Training Accuracy= 0.40625\n",
            "Iter 61, Loss= 1.294359, Training Accuracy= 0.45312\n",
            "Iter 61, Loss= 1.322585, Training Accuracy= 0.32812\n",
            "Iter 61, Loss= 1.326720, Training Accuracy= 0.29688\n",
            "Iter 61, Loss= 1.319734, Training Accuracy= 0.37500\n",
            "Iter 61, Loss= 1.311663, Training Accuracy= 0.35938\n",
            "Iter 61, Loss= 1.313757, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 62, Loss= 1.285914, Training Accuracy= 0.46875\n",
            "Iter 62, Loss= 1.290674, Training Accuracy= 0.46875\n",
            "Iter 62, Loss= 1.294452, Training Accuracy= 0.45312\n",
            "Iter 62, Loss= 1.325959, Training Accuracy= 0.34375\n",
            "Iter 62, Loss= 1.298123, Training Accuracy= 0.42188\n",
            "Iter 62, Loss= 1.300218, Training Accuracy= 0.40625\n",
            "Iter 62, Loss= 1.301199, Training Accuracy= 0.39062\n",
            "Iter 62, Loss= 1.302318, Training Accuracy= 0.40625\n",
            "Iter 62, Loss= 1.290865, Training Accuracy= 0.45312\n",
            "Iter 62, Loss= 1.319010, Training Accuracy= 0.32812\n",
            "Iter 62, Loss= 1.323062, Training Accuracy= 0.29688\n",
            "Iter 62, Loss= 1.316060, Training Accuracy= 0.37500\n",
            "Iter 62, Loss= 1.308197, Training Accuracy= 0.35938\n",
            "Iter 62, Loss= 1.310223, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 63, Loss= 1.282677, Training Accuracy= 0.46875\n",
            "Iter 63, Loss= 1.287334, Training Accuracy= 0.46875\n",
            "Iter 63, Loss= 1.291137, Training Accuracy= 0.45312\n",
            "Iter 63, Loss= 1.322134, Training Accuracy= 0.34375\n",
            "Iter 63, Loss= 1.294494, Training Accuracy= 0.42188\n",
            "Iter 63, Loss= 1.296943, Training Accuracy= 0.40625\n",
            "Iter 63, Loss= 1.297893, Training Accuracy= 0.39062\n",
            "Iter 63, Loss= 1.298869, Training Accuracy= 0.40625\n",
            "Iter 63, Loss= 1.287387, Training Accuracy= 0.45312\n",
            "Iter 63, Loss= 1.315447, Training Accuracy= 0.32812\n",
            "Iter 63, Loss= 1.319427, Training Accuracy= 0.29688\n",
            "Iter 63, Loss= 1.312422, Training Accuracy= 0.37500\n",
            "Iter 63, Loss= 1.304779, Training Accuracy= 0.35938\n",
            "Iter 63, Loss= 1.306702, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 64, Loss= 1.279445, Training Accuracy= 0.46875\n",
            "Iter 64, Loss= 1.283986, Training Accuracy= 0.46875\n",
            "Iter 64, Loss= 1.287867, Training Accuracy= 0.45312\n",
            "Iter 64, Loss= 1.318310, Training Accuracy= 0.34375\n",
            "Iter 64, Loss= 1.290875, Training Accuracy= 0.42188\n",
            "Iter 64, Loss= 1.293684, Training Accuracy= 0.40625\n",
            "Iter 64, Loss= 1.294610, Training Accuracy= 0.39062\n",
            "Iter 64, Loss= 1.295453, Training Accuracy= 0.40625\n",
            "Iter 64, Loss= 1.283925, Training Accuracy= 0.45312\n",
            "Iter 64, Loss= 1.311907, Training Accuracy= 0.32812\n",
            "Iter 64, Loss= 1.315817, Training Accuracy= 0.29688\n",
            "Iter 64, Loss= 1.308773, Training Accuracy= 0.37500\n",
            "Iter 64, Loss= 1.301392, Training Accuracy= 0.35938\n",
            "Iter 64, Loss= 1.303221, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 65, Loss= 1.276221, Training Accuracy= 0.46875\n",
            "Iter 65, Loss= 1.280634, Training Accuracy= 0.46875\n",
            "Iter 65, Loss= 1.284608, Training Accuracy= 0.45312\n",
            "Iter 65, Loss= 1.314496, Training Accuracy= 0.34375\n",
            "Iter 65, Loss= 1.287280, Training Accuracy= 0.42188\n",
            "Iter 65, Loss= 1.290460, Training Accuracy= 0.40625\n",
            "Iter 65, Loss= 1.291377, Training Accuracy= 0.39062\n",
            "Iter 65, Loss= 1.292103, Training Accuracy= 0.40625\n",
            "Iter 65, Loss= 1.280483, Training Accuracy= 0.45312\n",
            "Iter 65, Loss= 1.308408, Training Accuracy= 0.32812\n",
            "Iter 65, Loss= 1.312213, Training Accuracy= 0.29688\n",
            "Iter 65, Loss= 1.305148, Training Accuracy= 0.37500\n",
            "Iter 65, Loss= 1.298030, Training Accuracy= 0.35938\n",
            "Iter 65, Loss= 1.299778, Training Accuracy= 0.35938\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.46000\n",
            "Iter 66, Loss= 1.273017, Training Accuracy= 0.46875\n",
            "Iter 66, Loss= 1.277289, Training Accuracy= 0.46875\n",
            "Iter 66, Loss= 1.281371, Training Accuracy= 0.45312\n",
            "Iter 66, Loss= 1.310700, Training Accuracy= 0.34375\n",
            "Iter 66, Loss= 1.283712, Training Accuracy= 0.42188\n",
            "Iter 66, Loss= 1.287300, Training Accuracy= 0.40625\n",
            "Iter 66, Loss= 1.288210, Training Accuracy= 0.39062\n",
            "Iter 66, Loss= 1.288809, Training Accuracy= 0.42188\n",
            "Iter 66, Loss= 1.277064, Training Accuracy= 0.46875\n",
            "Iter 66, Loss= 1.304971, Training Accuracy= 0.34375\n",
            "Iter 66, Loss= 1.308618, Training Accuracy= 0.31250\n",
            "Iter 66, Loss= 1.301574, Training Accuracy= 0.37500\n",
            "Iter 66, Loss= 1.294689, Training Accuracy= 0.39062\n",
            "Iter 66, Loss= 1.296337, Training Accuracy= 0.37500\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.47000\n",
            "Iter 67, Loss= 1.269866, Training Accuracy= 0.50000\n",
            "Iter 67, Loss= 1.273939, Training Accuracy= 0.50000\n",
            "Iter 67, Loss= 1.278157, Training Accuracy= 0.48438\n",
            "Iter 67, Loss= 1.306923, Training Accuracy= 0.34375\n",
            "Iter 67, Loss= 1.280163, Training Accuracy= 0.46875\n",
            "Iter 67, Loss= 1.284208, Training Accuracy= 0.42188\n",
            "Iter 67, Loss= 1.285108, Training Accuracy= 0.39062\n",
            "Iter 67, Loss= 1.285594, Training Accuracy= 0.42188\n",
            "Iter 67, Loss= 1.273699, Training Accuracy= 0.50000\n",
            "Iter 67, Loss= 1.301609, Training Accuracy= 0.34375\n",
            "Iter 67, Loss= 1.305052, Training Accuracy= 0.34375\n",
            "Iter 67, Loss= 1.298064, Training Accuracy= 0.39062\n",
            "Iter 67, Loss= 1.291388, Training Accuracy= 0.43750\n",
            "Iter 67, Loss= 1.292990, Training Accuracy= 0.42188\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.50000\n",
            "Iter 68, Loss= 1.266763, Training Accuracy= 0.50000\n",
            "Iter 68, Loss= 1.270626, Training Accuracy= 0.53125\n",
            "Iter 68, Loss= 1.274968, Training Accuracy= 0.50000\n",
            "Iter 68, Loss= 1.303211, Training Accuracy= 0.34375\n",
            "Iter 68, Loss= 1.276674, Training Accuracy= 0.53125\n",
            "Iter 68, Loss= 1.281203, Training Accuracy= 0.46875\n",
            "Iter 68, Loss= 1.282105, Training Accuracy= 0.42188\n",
            "Iter 68, Loss= 1.282425, Training Accuracy= 0.43750\n",
            "Iter 68, Loss= 1.270445, Training Accuracy= 0.50000\n",
            "Iter 68, Loss= 1.298325, Training Accuracy= 0.40625\n",
            "Iter 68, Loss= 1.301578, Training Accuracy= 0.43750\n",
            "Iter 68, Loss= 1.294626, Training Accuracy= 0.43750\n",
            "Iter 68, Loss= 1.288166, Training Accuracy= 0.48438\n",
            "Iter 68, Loss= 1.289755, Training Accuracy= 0.46875\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.56000\n",
            "Iter 69, Loss= 1.263715, Training Accuracy= 0.56250\n",
            "Iter 69, Loss= 1.267360, Training Accuracy= 0.59375\n",
            "Iter 69, Loss= 1.271832, Training Accuracy= 0.54688\n",
            "Iter 69, Loss= 1.299559, Training Accuracy= 0.37500\n",
            "Iter 69, Loss= 1.273284, Training Accuracy= 0.56250\n",
            "Iter 69, Loss= 1.278245, Training Accuracy= 0.48438\n",
            "Iter 69, Loss= 1.279183, Training Accuracy= 0.42188\n",
            "Iter 69, Loss= 1.279380, Training Accuracy= 0.46875\n",
            "Iter 69, Loss= 1.267224, Training Accuracy= 0.53125\n",
            "Iter 69, Loss= 1.295093, Training Accuracy= 0.45312\n",
            "Iter 69, Loss= 1.298181, Training Accuracy= 0.48438\n",
            "Iter 69, Loss= 1.291262, Training Accuracy= 0.46875\n",
            "Iter 69, Loss= 1.284966, Training Accuracy= 0.53125\n",
            "Iter 69, Loss= 1.286647, Training Accuracy= 0.51562\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.63000\n",
            "Iter 70, Loss= 1.260749, Training Accuracy= 0.59375\n",
            "Iter 70, Loss= 1.264129, Training Accuracy= 0.67188\n",
            "Iter 70, Loss= 1.268765, Training Accuracy= 0.59375\n",
            "Iter 70, Loss= 1.296012, Training Accuracy= 0.43750\n",
            "Iter 70, Loss= 1.270040, Training Accuracy= 0.64062\n",
            "Iter 70, Loss= 1.275386, Training Accuracy= 0.50000\n",
            "Iter 70, Loss= 1.276327, Training Accuracy= 0.48438\n",
            "Iter 70, Loss= 1.276413, Training Accuracy= 0.48438\n",
            "Iter 70, Loss= 1.264095, Training Accuracy= 0.62500\n",
            "Iter 70, Loss= 1.291925, Training Accuracy= 0.51562\n",
            "Iter 70, Loss= 1.294832, Training Accuracy= 0.50000\n",
            "Iter 70, Loss= 1.287995, Training Accuracy= 0.51562\n",
            "Iter 70, Loss= 1.281822, Training Accuracy= 0.57812\n",
            "Iter 70, Loss= 1.283668, Training Accuracy= 0.53125\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.65000\n",
            "Iter 71, Loss= 1.257851, Training Accuracy= 0.65625\n",
            "Iter 71, Loss= 1.261003, Training Accuracy= 0.68750\n",
            "Iter 71, Loss= 1.265852, Training Accuracy= 0.62500\n",
            "Iter 71, Loss= 1.292567, Training Accuracy= 0.50000\n",
            "Iter 71, Loss= 1.266925, Training Accuracy= 0.67188\n",
            "Iter 71, Loss= 1.272622, Training Accuracy= 0.53125\n",
            "Iter 71, Loss= 1.273560, Training Accuracy= 0.51562\n",
            "Iter 71, Loss= 1.273497, Training Accuracy= 0.50000\n",
            "Iter 71, Loss= 1.261120, Training Accuracy= 0.64062\n",
            "Iter 71, Loss= 1.288853, Training Accuracy= 0.51562\n",
            "Iter 71, Loss= 1.291579, Training Accuracy= 0.53125\n",
            "Iter 71, Loss= 1.284859, Training Accuracy= 0.51562\n",
            "Iter 71, Loss= 1.278749, Training Accuracy= 0.59375\n",
            "Iter 71, Loss= 1.280805, Training Accuracy= 0.54688\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.69000\n",
            "Iter 72, Loss= 1.255074, Training Accuracy= 0.65625\n",
            "Iter 72, Loss= 1.258035, Training Accuracy= 0.68750\n",
            "Iter 72, Loss= 1.263047, Training Accuracy= 0.64062\n",
            "Iter 72, Loss= 1.289282, Training Accuracy= 0.51562\n",
            "Iter 72, Loss= 1.263897, Training Accuracy= 0.67188\n",
            "Iter 72, Loss= 1.269984, Training Accuracy= 0.54688\n",
            "Iter 72, Loss= 1.270910, Training Accuracy= 0.53125\n",
            "Iter 72, Loss= 1.270715, Training Accuracy= 0.51562\n",
            "Iter 72, Loss= 1.258292, Training Accuracy= 0.64062\n",
            "Iter 72, Loss= 1.285890, Training Accuracy= 0.53125\n",
            "Iter 72, Loss= 1.288411, Training Accuracy= 0.56250\n",
            "Iter 72, Loss= 1.281871, Training Accuracy= 0.51562\n",
            "Iter 72, Loss= 1.275778, Training Accuracy= 0.60938\n",
            "Iter 72, Loss= 1.278028, Training Accuracy= 0.57812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.69000\n",
            "Iter 73, Loss= 1.252436, Training Accuracy= 0.67188\n",
            "Iter 73, Loss= 1.255191, Training Accuracy= 0.68750\n",
            "Iter 73, Loss= 1.260397, Training Accuracy= 0.64062\n",
            "Iter 73, Loss= 1.286196, Training Accuracy= 0.53125\n",
            "Iter 73, Loss= 1.260953, Training Accuracy= 0.67188\n",
            "Iter 73, Loss= 1.267477, Training Accuracy= 0.56250\n",
            "Iter 73, Loss= 1.268352, Training Accuracy= 0.54688\n",
            "Iter 73, Loss= 1.268025, Training Accuracy= 0.51562\n",
            "Iter 73, Loss= 1.255556, Training Accuracy= 0.64062\n",
            "Iter 73, Loss= 1.282965, Training Accuracy= 0.53125\n",
            "Iter 73, Loss= 1.285322, Training Accuracy= 0.57812\n",
            "Iter 73, Loss= 1.279016, Training Accuracy= 0.51562\n",
            "Iter 73, Loss= 1.272899, Training Accuracy= 0.62500\n",
            "Iter 73, Loss= 1.275331, Training Accuracy= 0.57812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.69000\n",
            "Iter 74, Loss= 1.249913, Training Accuracy= 0.67188\n",
            "Iter 74, Loss= 1.252485, Training Accuracy= 0.68750\n",
            "Iter 74, Loss= 1.257801, Training Accuracy= 0.64062\n",
            "Iter 74, Loss= 1.283255, Training Accuracy= 0.53125\n",
            "Iter 74, Loss= 1.258088, Training Accuracy= 0.67188\n",
            "Iter 74, Loss= 1.265030, Training Accuracy= 0.56250\n",
            "Iter 74, Loss= 1.265846, Training Accuracy= 0.54688\n",
            "Iter 74, Loss= 1.265389, Training Accuracy= 0.51562\n",
            "Iter 74, Loss= 1.252912, Training Accuracy= 0.64062\n",
            "Iter 74, Loss= 1.280111, Training Accuracy= 0.53125\n",
            "Iter 74, Loss= 1.282304, Training Accuracy= 0.57812\n",
            "Iter 74, Loss= 1.276279, Training Accuracy= 0.51562\n",
            "Iter 74, Loss= 1.270096, Training Accuracy= 0.62500\n",
            "Iter 74, Loss= 1.272699, Training Accuracy= 0.57812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.69000\n",
            "Iter 75, Loss= 1.247478, Training Accuracy= 0.67188\n",
            "Iter 75, Loss= 1.249890, Training Accuracy= 0.68750\n",
            "Iter 75, Loss= 1.255268, Training Accuracy= 0.64062\n",
            "Iter 75, Loss= 1.280373, Training Accuracy= 0.53125\n",
            "Iter 75, Loss= 1.255307, Training Accuracy= 0.67188\n",
            "Iter 75, Loss= 1.262637, Training Accuracy= 0.56250\n",
            "Iter 75, Loss= 1.263384, Training Accuracy= 0.54688\n",
            "Iter 75, Loss= 1.262815, Training Accuracy= 0.51562\n",
            "Iter 75, Loss= 1.250325, Training Accuracy= 0.64062\n",
            "Iter 75, Loss= 1.277353, Training Accuracy= 0.53125\n",
            "Iter 75, Loss= 1.279360, Training Accuracy= 0.57812\n",
            "Iter 75, Loss= 1.273620, Training Accuracy= 0.51562\n",
            "Iter 75, Loss= 1.267355, Training Accuracy= 0.62500\n",
            "Iter 75, Loss= 1.270129, Training Accuracy= 0.57812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.69000\n",
            "Iter 76, Loss= 1.245103, Training Accuracy= 0.67188\n",
            "Iter 76, Loss= 1.247355, Training Accuracy= 0.68750\n",
            "Iter 76, Loss= 1.252794, Training Accuracy= 0.64062\n",
            "Iter 76, Loss= 1.277559, Training Accuracy= 0.53125\n",
            "Iter 76, Loss= 1.252593, Training Accuracy= 0.67188\n",
            "Iter 76, Loss= 1.260291, Training Accuracy= 0.56250\n",
            "Iter 76, Loss= 1.260951, Training Accuracy= 0.54688\n",
            "Iter 76, Loss= 1.260297, Training Accuracy= 0.51562\n",
            "Iter 76, Loss= 1.247785, Training Accuracy= 0.64062\n",
            "Iter 76, Loss= 1.274654, Training Accuracy= 0.53125\n",
            "Iter 76, Loss= 1.276464, Training Accuracy= 0.57812\n",
            "Iter 76, Loss= 1.271020, Training Accuracy= 0.51562\n",
            "Iter 76, Loss= 1.264703, Training Accuracy= 0.62500\n",
            "Iter 76, Loss= 1.267608, Training Accuracy= 0.57812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.69000\n",
            "Iter 77, Loss= 1.242777, Training Accuracy= 0.67188\n",
            "Iter 77, Loss= 1.244888, Training Accuracy= 0.68750\n",
            "Iter 77, Loss= 1.250377, Training Accuracy= 0.64062\n",
            "Iter 77, Loss= 1.274798, Training Accuracy= 0.53125\n",
            "Iter 77, Loss= 1.249950, Training Accuracy= 0.67188\n",
            "Iter 77, Loss= 1.257988, Training Accuracy= 0.56250\n",
            "Iter 77, Loss= 1.258558, Training Accuracy= 0.54688\n",
            "Iter 77, Loss= 1.257824, Training Accuracy= 0.51562\n",
            "Iter 77, Loss= 1.245297, Training Accuracy= 0.64062\n",
            "Iter 77, Loss= 1.272000, Training Accuracy= 0.53125\n",
            "Iter 77, Loss= 1.273607, Training Accuracy= 0.57812\n",
            "Iter 77, Loss= 1.268477, Training Accuracy= 0.51562\n",
            "Iter 77, Loss= 1.262099, Training Accuracy= 0.62500\n",
            "Iter 77, Loss= 1.265132, Training Accuracy= 0.57812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.69000\n",
            "Iter 78, Loss= 1.240504, Training Accuracy= 0.67188\n",
            "Iter 78, Loss= 1.242506, Training Accuracy= 0.68750\n",
            "Iter 78, Loss= 1.248016, Training Accuracy= 0.64062\n",
            "Iter 78, Loss= 1.272098, Training Accuracy= 0.53125\n",
            "Iter 78, Loss= 1.247352, Training Accuracy= 0.67188\n",
            "Iter 78, Loss= 1.255723, Training Accuracy= 0.56250\n",
            "Iter 78, Loss= 1.256200, Training Accuracy= 0.54688\n",
            "Iter 78, Loss= 1.255409, Training Accuracy= 0.51562\n",
            "Iter 78, Loss= 1.242861, Training Accuracy= 0.64062\n",
            "Iter 78, Loss= 1.269391, Training Accuracy= 0.53125\n",
            "Iter 78, Loss= 1.270794, Training Accuracy= 0.57812\n",
            "Iter 78, Loss= 1.265972, Training Accuracy= 0.51562\n",
            "Iter 78, Loss= 1.259538, Training Accuracy= 0.62500\n",
            "Iter 78, Loss= 1.262694, Training Accuracy= 0.57812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.69000\n",
            "Iter 79, Loss= 1.238264, Training Accuracy= 0.67188\n",
            "Iter 79, Loss= 1.240167, Training Accuracy= 0.68750\n",
            "Iter 79, Loss= 1.245703, Training Accuracy= 0.64062\n",
            "Iter 79, Loss= 1.269441, Training Accuracy= 0.53125\n",
            "Iter 79, Loss= 1.244803, Training Accuracy= 0.67188\n",
            "Iter 79, Loss= 1.253486, Training Accuracy= 0.56250\n",
            "Iter 79, Loss= 1.253872, Training Accuracy= 0.54688\n",
            "Iter 79, Loss= 1.253038, Training Accuracy= 0.51562\n",
            "Iter 79, Loss= 1.240474, Training Accuracy= 0.64062\n",
            "Iter 79, Loss= 1.266826, Training Accuracy= 0.53125\n",
            "Iter 79, Loss= 1.268025, Training Accuracy= 0.57812\n",
            "Iter 79, Loss= 1.263503, Training Accuracy= 0.53125\n",
            "Iter 79, Loss= 1.257010, Training Accuracy= 0.62500\n",
            "Iter 79, Loss= 1.260285, Training Accuracy= 0.57812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.69000\n",
            "Iter 80, Loss= 1.236057, Training Accuracy= 0.67188\n",
            "Iter 80, Loss= 1.237866, Training Accuracy= 0.68750\n",
            "Iter 80, Loss= 1.243422, Training Accuracy= 0.64062\n",
            "Iter 80, Loss= 1.266822, Training Accuracy= 0.53125\n",
            "Iter 80, Loss= 1.242303, Training Accuracy= 0.67188\n",
            "Iter 80, Loss= 1.251276, Training Accuracy= 0.56250\n",
            "Iter 80, Loss= 1.251579, Training Accuracy= 0.56250\n",
            "Iter 80, Loss= 1.250699, Training Accuracy= 0.51562\n",
            "Iter 80, Loss= 1.238130, Training Accuracy= 0.64062\n",
            "Iter 80, Loss= 1.264303, Training Accuracy= 0.53125\n",
            "Iter 80, Loss= 1.265299, Training Accuracy= 0.57812\n",
            "Iter 80, Loss= 1.261047, Training Accuracy= 0.53125\n",
            "Iter 80, Loss= 1.254518, Training Accuracy= 0.62500\n",
            "Iter 80, Loss= 1.257896, Training Accuracy= 0.57812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.69000\n",
            "Iter 81, Loss= 1.233881, Training Accuracy= 0.67188\n",
            "Iter 81, Loss= 1.235599, Training Accuracy= 0.68750\n",
            "Iter 81, Loss= 1.241158, Training Accuracy= 0.64062\n",
            "Iter 81, Loss= 1.264246, Training Accuracy= 0.53125\n",
            "Iter 81, Loss= 1.239843, Training Accuracy= 0.67188\n",
            "Iter 81, Loss= 1.249096, Training Accuracy= 0.56250\n",
            "Iter 81, Loss= 1.249316, Training Accuracy= 0.56250\n",
            "Iter 81, Loss= 1.248411, Training Accuracy= 0.51562\n",
            "Iter 81, Loss= 1.235827, Training Accuracy= 0.64062\n",
            "Iter 81, Loss= 1.261814, Training Accuracy= 0.53125\n",
            "Iter 81, Loss= 1.262608, Training Accuracy= 0.57812\n",
            "Iter 81, Loss= 1.258620, Training Accuracy= 0.54688\n",
            "Iter 81, Loss= 1.252054, Training Accuracy= 0.62500\n",
            "Iter 81, Loss= 1.255532, Training Accuracy= 0.57812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.69000\n",
            "Iter 82, Loss= 1.231733, Training Accuracy= 0.67188\n",
            "Iter 82, Loss= 1.233348, Training Accuracy= 0.70312\n",
            "Iter 82, Loss= 1.238922, Training Accuracy= 0.65625\n",
            "Iter 82, Loss= 1.261694, Training Accuracy= 0.53125\n",
            "Iter 82, Loss= 1.237415, Training Accuracy= 0.67188\n",
            "Iter 82, Loss= 1.246942, Training Accuracy= 0.56250\n",
            "Iter 82, Loss= 1.247078, Training Accuracy= 0.56250\n",
            "Iter 82, Loss= 1.246153, Training Accuracy= 0.54688\n",
            "Iter 82, Loss= 1.233557, Training Accuracy= 0.64062\n",
            "Iter 82, Loss= 1.259358, Training Accuracy= 0.54688\n",
            "Iter 82, Loss= 1.259941, Training Accuracy= 0.57812\n",
            "Iter 82, Loss= 1.256223, Training Accuracy= 0.54688\n",
            "Iter 82, Loss= 1.249605, Training Accuracy= 0.62500\n",
            "Iter 82, Loss= 1.253188, Training Accuracy= 0.57812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.71000\n",
            "Iter 83, Loss= 1.229609, Training Accuracy= 0.67188\n",
            "Iter 83, Loss= 1.231123, Training Accuracy= 0.70312\n",
            "Iter 83, Loss= 1.236702, Training Accuracy= 0.67188\n",
            "Iter 83, Loss= 1.259169, Training Accuracy= 0.54688\n",
            "Iter 83, Loss= 1.235023, Training Accuracy= 0.67188\n",
            "Iter 83, Loss= 1.244791, Training Accuracy= 0.56250\n",
            "Iter 83, Loss= 1.244853, Training Accuracy= 0.57812\n",
            "Iter 83, Loss= 1.243912, Training Accuracy= 0.59375\n",
            "Iter 83, Loss= 1.231323, Training Accuracy= 0.64062\n",
            "Iter 83, Loss= 1.256933, Training Accuracy= 0.56250\n",
            "Iter 83, Loss= 1.257300, Training Accuracy= 0.59375\n",
            "Iter 83, Loss= 1.253856, Training Accuracy= 0.57812\n",
            "Iter 83, Loss= 1.247150, Training Accuracy= 0.64062\n",
            "Iter 83, Loss= 1.250867, Training Accuracy= 0.57812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.71000\n",
            "Iter 84, Loss= 1.227499, Training Accuracy= 0.67188\n",
            "Iter 84, Loss= 1.228920, Training Accuracy= 0.73438\n",
            "Iter 84, Loss= 1.234485, Training Accuracy= 0.70312\n",
            "Iter 84, Loss= 1.256688, Training Accuracy= 0.57812\n",
            "Iter 84, Loss= 1.232658, Training Accuracy= 0.67188\n",
            "Iter 84, Loss= 1.242648, Training Accuracy= 0.59375\n",
            "Iter 84, Loss= 1.242638, Training Accuracy= 0.59375\n",
            "Iter 84, Loss= 1.241688, Training Accuracy= 0.60938\n",
            "Iter 84, Loss= 1.229113, Training Accuracy= 0.64062\n",
            "Iter 84, Loss= 1.254530, Training Accuracy= 0.57812\n",
            "Iter 84, Loss= 1.254680, Training Accuracy= 0.60938\n",
            "Iter 84, Loss= 1.251495, Training Accuracy= 0.65625\n",
            "Iter 84, Loss= 1.244708, Training Accuracy= 0.67188\n",
            "Iter 84, Loss= 1.248554, Training Accuracy= 0.64062\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.73000\n",
            "Iter 85, Loss= 1.225414, Training Accuracy= 0.68750\n",
            "Iter 85, Loss= 1.226721, Training Accuracy= 0.73438\n",
            "Iter 85, Loss= 1.232278, Training Accuracy= 0.76562\n",
            "Iter 85, Loss= 1.254235, Training Accuracy= 0.62500\n",
            "Iter 85, Loss= 1.230321, Training Accuracy= 0.68750\n",
            "Iter 85, Loss= 1.240512, Training Accuracy= 0.65625\n",
            "Iter 85, Loss= 1.240420, Training Accuracy= 0.67188\n",
            "Iter 85, Loss= 1.239480, Training Accuracy= 0.65625\n",
            "Iter 85, Loss= 1.226916, Training Accuracy= 0.71875\n",
            "Iter 85, Loss= 1.252144, Training Accuracy= 0.71875\n",
            "Iter 85, Loss= 1.252089, Training Accuracy= 0.67188\n",
            "Iter 85, Loss= 1.249152, Training Accuracy= 0.75000\n",
            "Iter 85, Loss= 1.242284, Training Accuracy= 0.78125\n",
            "Iter 85, Loss= 1.246237, Training Accuracy= 0.79688\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.82000\n",
            "Iter 86, Loss= 1.223334, Training Accuracy= 0.84375\n",
            "Iter 86, Loss= 1.224526, Training Accuracy= 0.89062\n",
            "Iter 86, Loss= 1.230076, Training Accuracy= 0.84375\n",
            "Iter 86, Loss= 1.251805, Training Accuracy= 0.75000\n",
            "Iter 86, Loss= 1.228006, Training Accuracy= 0.76562\n",
            "Iter 86, Loss= 1.238373, Training Accuracy= 0.82812\n",
            "Iter 86, Loss= 1.238194, Training Accuracy= 0.78125\n",
            "Iter 86, Loss= 1.237275, Training Accuracy= 0.73438\n",
            "Iter 86, Loss= 1.224738, Training Accuracy= 0.78125\n",
            "Iter 86, Loss= 1.249765, Training Accuracy= 0.76562\n",
            "Iter 86, Loss= 1.249521, Training Accuracy= 0.70312\n",
            "Iter 86, Loss= 1.246805, Training Accuracy= 0.79688\n",
            "Iter 86, Loss= 1.239876, Training Accuracy= 0.78125\n",
            "Iter 86, Loss= 1.243915, Training Accuracy= 0.82812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.83000\n",
            "Iter 87, Loss= 1.221259, Training Accuracy= 0.85938\n",
            "Iter 87, Loss= 1.222336, Training Accuracy= 0.89062\n",
            "Iter 87, Loss= 1.227880, Training Accuracy= 0.87500\n",
            "Iter 87, Loss= 1.249371, Training Accuracy= 0.76562\n",
            "Iter 87, Loss= 1.225701, Training Accuracy= 0.76562\n",
            "Iter 87, Loss= 1.236202, Training Accuracy= 0.82812\n",
            "Iter 87, Loss= 1.235973, Training Accuracy= 0.78125\n",
            "Iter 87, Loss= 1.235058, Training Accuracy= 0.73438\n",
            "Iter 87, Loss= 1.222588, Training Accuracy= 0.78125\n",
            "Iter 87, Loss= 1.247374, Training Accuracy= 0.76562\n",
            "Iter 87, Loss= 1.246969, Training Accuracy= 0.70312\n",
            "Iter 87, Loss= 1.244452, Training Accuracy= 0.79688\n",
            "Iter 87, Loss= 1.237490, Training Accuracy= 0.78125\n",
            "Iter 87, Loss= 1.241583, Training Accuracy= 0.82812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.83000\n",
            "Iter 88, Loss= 1.219171, Training Accuracy= 0.85938\n",
            "Iter 88, Loss= 1.220133, Training Accuracy= 0.89062\n",
            "Iter 88, Loss= 1.225677, Training Accuracy= 0.87500\n",
            "Iter 88, Loss= 1.246940, Training Accuracy= 0.76562\n",
            "Iter 88, Loss= 1.223413, Training Accuracy= 0.76562\n",
            "Iter 88, Loss= 1.233983, Training Accuracy= 0.82812\n",
            "Iter 88, Loss= 1.233746, Training Accuracy= 0.78125\n",
            "Iter 88, Loss= 1.232832, Training Accuracy= 0.73438\n",
            "Iter 88, Loss= 1.220453, Training Accuracy= 0.78125\n",
            "Iter 88, Loss= 1.244935, Training Accuracy= 0.76562\n",
            "Iter 88, Loss= 1.244427, Training Accuracy= 0.70312\n",
            "Iter 88, Loss= 1.242079, Training Accuracy= 0.79688\n",
            "Iter 88, Loss= 1.235122, Training Accuracy= 0.78125\n",
            "Iter 88, Loss= 1.239226, Training Accuracy= 0.82812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.83000\n",
            "Iter 89, Loss= 1.217068, Training Accuracy= 0.85938\n",
            "Iter 89, Loss= 1.217918, Training Accuracy= 0.89062\n",
            "Iter 89, Loss= 1.223469, Training Accuracy= 0.87500\n",
            "Iter 89, Loss= 1.244488, Training Accuracy= 0.76562\n",
            "Iter 89, Loss= 1.221142, Training Accuracy= 0.76562\n",
            "Iter 89, Loss= 1.231743, Training Accuracy= 0.82812\n",
            "Iter 89, Loss= 1.231477, Training Accuracy= 0.78125\n",
            "Iter 89, Loss= 1.230587, Training Accuracy= 0.73438\n",
            "Iter 89, Loss= 1.218336, Training Accuracy= 0.78125\n",
            "Iter 89, Loss= 1.242489, Training Accuracy= 0.76562\n",
            "Iter 89, Loss= 1.241889, Training Accuracy= 0.70312\n",
            "Iter 89, Loss= 1.239670, Training Accuracy= 0.79688\n",
            "Iter 89, Loss= 1.232738, Training Accuracy= 0.78125\n",
            "Iter 89, Loss= 1.236867, Training Accuracy= 0.82812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.83000\n",
            "Iter 90, Loss= 1.214943, Training Accuracy= 0.85938\n",
            "Iter 90, Loss= 1.215648, Training Accuracy= 0.89062\n",
            "Iter 90, Loss= 1.221235, Training Accuracy= 0.87500\n",
            "Iter 90, Loss= 1.242018, Training Accuracy= 0.76562\n",
            "Iter 90, Loss= 1.218851, Training Accuracy= 0.76562\n",
            "Iter 90, Loss= 1.229451, Training Accuracy= 0.82812\n",
            "Iter 90, Loss= 1.229155, Training Accuracy= 0.78125\n",
            "Iter 90, Loss= 1.228328, Training Accuracy= 0.73438\n",
            "Iter 90, Loss= 1.216194, Training Accuracy= 0.78125\n",
            "Iter 90, Loss= 1.240035, Training Accuracy= 0.76562\n",
            "Iter 90, Loss= 1.239359, Training Accuracy= 0.70312\n",
            "Iter 90, Loss= 1.237241, Training Accuracy= 0.79688\n",
            "Iter 90, Loss= 1.230325, Training Accuracy= 0.78125\n",
            "Iter 90, Loss= 1.234450, Training Accuracy= 0.82812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.83000\n",
            "Iter 91, Loss= 1.212803, Training Accuracy= 0.85938\n",
            "Iter 91, Loss= 1.213359, Training Accuracy= 0.89062\n",
            "Iter 91, Loss= 1.218974, Training Accuracy= 0.87500\n",
            "Iter 91, Loss= 1.239521, Training Accuracy= 0.76562\n",
            "Iter 91, Loss= 1.216552, Training Accuracy= 0.76562\n",
            "Iter 91, Loss= 1.227127, Training Accuracy= 0.82812\n",
            "Iter 91, Loss= 1.226810, Training Accuracy= 0.78125\n",
            "Iter 91, Loss= 1.226052, Training Accuracy= 0.73438\n",
            "Iter 91, Loss= 1.214030, Training Accuracy= 0.78125\n",
            "Iter 91, Loss= 1.237557, Training Accuracy= 0.76562\n",
            "Iter 91, Loss= 1.236821, Training Accuracy= 0.70312\n",
            "Iter 91, Loss= 1.234802, Training Accuracy= 0.79688\n",
            "Iter 91, Loss= 1.227878, Training Accuracy= 0.78125\n",
            "Iter 91, Loss= 1.231982, Training Accuracy= 0.82812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.83000\n",
            "Iter 92, Loss= 1.210624, Training Accuracy= 0.85938\n",
            "Iter 92, Loss= 1.211009, Training Accuracy= 0.89062\n",
            "Iter 92, Loss= 1.216682, Training Accuracy= 0.87500\n",
            "Iter 92, Loss= 1.236986, Training Accuracy= 0.76562\n",
            "Iter 92, Loss= 1.214242, Training Accuracy= 0.76562\n",
            "Iter 92, Loss= 1.224735, Training Accuracy= 0.82812\n",
            "Iter 92, Loss= 1.224433, Training Accuracy= 0.78125\n",
            "Iter 92, Loss= 1.223754, Training Accuracy= 0.73438\n",
            "Iter 92, Loss= 1.211834, Training Accuracy= 0.78125\n",
            "Iter 92, Loss= 1.235072, Training Accuracy= 0.76562\n",
            "Iter 92, Loss= 1.234275, Training Accuracy= 0.70312\n",
            "Iter 92, Loss= 1.232335, Training Accuracy= 0.79688\n",
            "Iter 92, Loss= 1.225401, Training Accuracy= 0.78125\n",
            "Iter 92, Loss= 1.229486, Training Accuracy= 0.82812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.83000\n",
            "Iter 93, Loss= 1.208388, Training Accuracy= 0.85938\n",
            "Iter 93, Loss= 1.208622, Training Accuracy= 0.89062\n",
            "Iter 93, Loss= 1.214358, Training Accuracy= 0.87500\n",
            "Iter 93, Loss= 1.234420, Training Accuracy= 0.76562\n",
            "Iter 93, Loss= 1.211932, Training Accuracy= 0.76562\n",
            "Iter 93, Loss= 1.222304, Training Accuracy= 0.82812\n",
            "Iter 93, Loss= 1.222011, Training Accuracy= 0.78125\n",
            "Iter 93, Loss= 1.221409, Training Accuracy= 0.73438\n",
            "Iter 93, Loss= 1.209629, Training Accuracy= 0.78125\n",
            "Iter 93, Loss= 1.232543, Training Accuracy= 0.76562\n",
            "Iter 93, Loss= 1.231699, Training Accuracy= 0.70312\n",
            "Iter 93, Loss= 1.229822, Training Accuracy= 0.79688\n",
            "Iter 93, Loss= 1.222888, Training Accuracy= 0.78125\n",
            "Iter 93, Loss= 1.226957, Training Accuracy= 0.82812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.83000\n",
            "Iter 94, Loss= 1.206050, Training Accuracy= 0.85938\n",
            "Iter 94, Loss= 1.206204, Training Accuracy= 0.89062\n",
            "Iter 94, Loss= 1.211990, Training Accuracy= 0.87500\n",
            "Iter 94, Loss= 1.231827, Training Accuracy= 0.76562\n",
            "Iter 94, Loss= 1.209604, Training Accuracy= 0.76562\n",
            "Iter 94, Loss= 1.219828, Training Accuracy= 0.82812\n",
            "Iter 94, Loss= 1.219548, Training Accuracy= 0.78125\n",
            "Iter 94, Loss= 1.219039, Training Accuracy= 0.73438\n",
            "Iter 94, Loss= 1.207393, Training Accuracy= 0.78125\n",
            "Iter 94, Loss= 1.229968, Training Accuracy= 0.76562\n",
            "Iter 94, Loss= 1.229115, Training Accuracy= 0.70312\n",
            "Iter 94, Loss= 1.227251, Training Accuracy= 0.79688\n",
            "Iter 94, Loss= 1.220364, Training Accuracy= 0.78125\n",
            "Iter 94, Loss= 1.224363, Training Accuracy= 0.82812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.83000\n",
            "Iter 95, Loss= 1.203707, Training Accuracy= 0.85938\n",
            "Iter 95, Loss= 1.203757, Training Accuracy= 0.89062\n",
            "Iter 95, Loss= 1.209577, Training Accuracy= 0.87500\n",
            "Iter 95, Loss= 1.229207, Training Accuracy= 0.76562\n",
            "Iter 95, Loss= 1.207260, Training Accuracy= 0.76562\n",
            "Iter 95, Loss= 1.217312, Training Accuracy= 0.82812\n",
            "Iter 95, Loss= 1.217065, Training Accuracy= 0.78125\n",
            "Iter 95, Loss= 1.216645, Training Accuracy= 0.73438\n",
            "Iter 95, Loss= 1.205133, Training Accuracy= 0.78125\n",
            "Iter 95, Loss= 1.227364, Training Accuracy= 0.76562\n",
            "Iter 95, Loss= 1.226514, Training Accuracy= 0.70312\n",
            "Iter 95, Loss= 1.224669, Training Accuracy= 0.79688\n",
            "Iter 95, Loss= 1.217826, Training Accuracy= 0.78125\n",
            "Iter 95, Loss= 1.221725, Training Accuracy= 0.82812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.83000\n",
            "Iter 96, Loss= 1.201357, Training Accuracy= 0.85938\n",
            "Iter 96, Loss= 1.201288, Training Accuracy= 0.89062\n",
            "Iter 96, Loss= 1.207131, Training Accuracy= 0.87500\n",
            "Iter 96, Loss= 1.226568, Training Accuracy= 0.76562\n",
            "Iter 96, Loss= 1.204911, Training Accuracy= 0.76562\n",
            "Iter 96, Loss= 1.214760, Training Accuracy= 0.82812\n",
            "Iter 96, Loss= 1.214570, Training Accuracy= 0.78125\n",
            "Iter 96, Loss= 1.214250, Training Accuracy= 0.73438\n",
            "Iter 96, Loss= 1.202851, Training Accuracy= 0.78125\n",
            "Iter 96, Loss= 1.224728, Training Accuracy= 0.76562\n",
            "Iter 96, Loss= 1.223906, Training Accuracy= 0.70312\n",
            "Iter 96, Loss= 1.222074, Training Accuracy= 0.79688\n",
            "Iter 96, Loss= 1.215278, Training Accuracy= 0.78125\n",
            "Iter 96, Loss= 1.219059, Training Accuracy= 0.82812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.83000\n",
            "Iter 97, Loss= 1.198963, Training Accuracy= 0.85938\n",
            "Iter 97, Loss= 1.198803, Training Accuracy= 0.89062\n",
            "Iter 97, Loss= 1.204640, Training Accuracy= 0.87500\n",
            "Iter 97, Loss= 1.223913, Training Accuracy= 0.76562\n",
            "Iter 97, Loss= 1.202568, Training Accuracy= 0.76562\n",
            "Iter 97, Loss= 1.212153, Training Accuracy= 0.82812\n",
            "Iter 97, Loss= 1.212061, Training Accuracy= 0.78125\n",
            "Iter 97, Loss= 1.211852, Training Accuracy= 0.73438\n",
            "Iter 97, Loss= 1.200541, Training Accuracy= 0.78125\n",
            "Iter 97, Loss= 1.222077, Training Accuracy= 0.76562\n",
            "Iter 97, Loss= 1.221292, Training Accuracy= 0.70312\n",
            "Iter 97, Loss= 1.219470, Training Accuracy= 0.79688\n",
            "Iter 97, Loss= 1.212714, Training Accuracy= 0.78125\n",
            "Iter 97, Loss= 1.216375, Training Accuracy= 0.82812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.83000\n",
            "Iter 98, Loss= 1.196537, Training Accuracy= 0.85938\n",
            "Iter 98, Loss= 1.196302, Training Accuracy= 0.89062\n",
            "Iter 98, Loss= 1.202118, Training Accuracy= 0.87500\n",
            "Iter 98, Loss= 1.221248, Training Accuracy= 0.76562\n",
            "Iter 98, Loss= 1.200219, Training Accuracy= 0.76562\n",
            "Iter 98, Loss= 1.209520, Training Accuracy= 0.82812\n",
            "Iter 98, Loss= 1.209546, Training Accuracy= 0.78125\n",
            "Iter 98, Loss= 1.209455, Training Accuracy= 0.73438\n",
            "Iter 98, Loss= 1.198219, Training Accuracy= 0.78125\n",
            "Iter 98, Loss= 1.219414, Training Accuracy= 0.76562\n",
            "Iter 98, Loss= 1.218680, Training Accuracy= 0.70312\n",
            "Iter 98, Loss= 1.216867, Training Accuracy= 0.79688\n",
            "Iter 98, Loss= 1.210145, Training Accuracy= 0.78125\n",
            "Iter 98, Loss= 1.213684, Training Accuracy= 0.82812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.83000\n",
            "Iter 99, Loss= 1.194092, Training Accuracy= 0.85938\n",
            "Iter 99, Loss= 1.193795, Training Accuracy= 0.89062\n",
            "Iter 99, Loss= 1.199599, Training Accuracy= 0.87500\n",
            "Iter 99, Loss= 1.218591, Training Accuracy= 0.76562\n",
            "Iter 99, Loss= 1.197870, Training Accuracy= 0.76562\n",
            "Iter 99, Loss= 1.206875, Training Accuracy= 0.82812\n",
            "Iter 99, Loss= 1.207040, Training Accuracy= 0.78125\n",
            "Iter 99, Loss= 1.207076, Training Accuracy= 0.73438\n",
            "Iter 99, Loss= 1.195890, Training Accuracy= 0.78125\n",
            "Iter 99, Loss= 1.216756, Training Accuracy= 0.76562\n",
            "Iter 99, Loss= 1.216079, Training Accuracy= 0.70312\n",
            "Iter 99, Loss= 1.214282, Training Accuracy= 0.79688\n",
            "Iter 99, Loss= 1.207579, Training Accuracy= 0.78125\n",
            "Iter 99, Loss= 1.210987, Training Accuracy= 0.82812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.83000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR6q6yKYmofL",
        "colab_type": "code",
        "outputId": "31fc8944-7e80-402e-c60c-68255871f830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "plt.figure(figsize = (10, 5))\n",
        "plt.subplot(121)\n",
        "plt.title('With batch normalization')\n",
        "\n",
        "plt.plot(range(100), training_accuracy, color  = 'blue', label = 'training')\n",
        "plt.plot(range(100), testing_accuracy,  color = 'orange',label =  'test')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.subplot(122)\n",
        "plt.title('With batch normalization')\n",
        "\n",
        "plt.plot(range(100), training_loss, color  = 'blue', label = 'training')\n",
        "plt.plot(range(100), testing_loss,  color = 'orange', label = 'test')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f9b60d8b940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAFNCAYAAACuWnPfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s63iq6lTycm2",
        "colab_type": "code",
        "outputId": "44db7908-a45f-40f9-9fac-36e68ea4ce2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "plt.figure(figsize = (10, 5))\n",
        "plt.subplot(121)\n",
        "plt.title('Without normalization')\n",
        "\n",
        "plt.plot(range(100), training_accuracy, color  = 'blue', label = 'training')\n",
        "plt.plot(range(100), testing_accuracy,  color = 'orange',label =  'test')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.subplot(122)\n",
        "plt.title('Without normalization')\n",
        "\n",
        "plt.plot(range(100), training_loss, color  = 'blue', label = 'training')\n",
        "plt.plot(range(100), testing_loss,  color = 'orange', label = 'test')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6af479afd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAFNCAYAAACuWnPfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03XnMfi9z0_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "d21efbdb-6c91-4a5e-d57f-75ba723521ed"
      },
      "source": [
        "plt.figure(figsize = (10, 5))\n",
        "plt.subplot(121)\n",
        "plt.title('With layer normalization')\n",
        "\n",
        "plt.plot(range(100), training_accuracy, color  = 'blue', label = 'training')\n",
        "plt.plot(range(100), testing_accuracy,  color = 'orange',label =  'test')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.subplot(122)\n",
        "plt.title('With layer normalization')\n",
        "\n",
        "plt.plot(range(100), training_loss, color  = 'blue', label = 'training')\n",
        "plt.plot(range(100), testing_loss,  color = 'orange', label = 'test')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f206b5261d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAFNCAYAAACuWnPfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkfRvicysm0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# COMMENTS\n",
        "# Doubt: Do we fix normalisation for preactivation or not?\n",
        "# First of all, the results are very stochastic.\n",
        "# Nonetheless, batch norm seems to be marginally better than performance without batch norm; training is accelarated\n",
        "# layer nor doesnt seem to be accelarating performance; though sometimes superior accuracy is achieved using layer norm"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}